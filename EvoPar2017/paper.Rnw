\documentclass[runningheads,a4paper]{llncs}

\usepackage[latin1]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{subfigure}
\usepackage{url}
\usepackage{caption}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\providecommand{\tabularnewline}{\\}

\graphicspath{{../img/}}
\DeclareGraphicsExtensions{.pdf}


\begin{document}
%\SweaveOpts{concordance=TRUE}

<<setup, cache=FALSE,echo=FALSE>>=
library("ggplot2")
library("RColorBrewer")

experiment.data <- data.frame(Experiment=character(),
                              milliseconds=character(),                           
                              IPs=character(), 
                              actualIPs=character(),
                              PUTs=character(),
                              actualPUTs=character(),
                              PUTratio = character(),
                              PUTsIP =character())

these.experiments <- c("nooverlap-1","nooverlap-newgraph-1", "nooverlap-newgraph-2","nooverlap-newgraph-reboot","nooverlap-newgraph-reboot-cachecrash")

experiment.aggregates <- data.frame(Experiment=character(),
                                    milliseconds=character(),
                                    IPs=character(),
                                    actualIPs = character(),
                                    PUTs = character(),
                                    actualPUTs = character(),
                                    count = character(),
                                    totalIPs = character(),
                                    PUTsIP = character())

experiment.aggregates.filtered = experiment.aggregates

ips.data <- data.frame(Experiment=character(),                      
                       PUTs=character(),
                       reboots=character(),
                       idu=character(),
                       idx=character())

ips.aggregates <- data.frame(Experiment=character(),
                             PUTs = character(),
                             reboots = character(),
                             avgPUTs = character(),
                             avgreboots = character(),
                             totalIPs = character())

for ( i in these.experiments ) {
    this.data <- read.csv(paste0("../data/2016-PPSN/ips-time-cache-2016-03-cache=32-",i,".csv"))

    this.ip.data <- read.csv(paste0("../data/2016-PPSN/ips-reboots-cache=32-",i,".csv"))
    this.len <- length(this.ip.data$PUTs)
    seq <- 1:this.len
    ips.data <- rbind(ips.data,
                      data.frame(Experiment=rep(i,length(this.ip.data$PUTs)),
                                 idu=seq,
                                 idx=seq/this.len,
                                 PUTs=as.integer(this.ip.data$PUTs),
                                 reboots=as.integer(this.ip.data$reboots))
                       )
    
    ips.aggregates <- rbind( ips.aggregates,
                            data.frame(Experiment=i,
                                       PUTs = median(this.ip.data$PUTs),
                                       reboots = median(this.ip.data$reboots),
                                       avgPUTs = mean(this.ip.data$PUTs),
                                       avgreboots = mean(this.ip.data$reboots),
                                       totalIPs =this.len )
                            )
    
    experiment.aggregates <- rbind( experiment.aggregates,
                                   data.frame( Experiment=i,                   
                                              milliseconds=median(as.integer(this.data$milliseconds)),
                                              IPs=median(as.integer(this.data$IPs)),
                                              actualIPs=median(as.integer(this.data$actualIPs)),
                                              PUTs=median(as.integer(this.data$PUTs)),
                                              actualPUTs=median(as.integer(this.data$actualPUTs)),
                                              PUTratio=median(as.integer(this.data$actualPUTs)/as.integer(this.data$PUTs)),
                                              count= length(this.data$IPs),
                                              totalIPs=this.len,
                                              PUTsIP=median(as.integer(this.data$PUTs)/as.integer(this.data$IPs)))
                                   )
    
    experiment.data <- rbind( experiment.data,
                             data.frame( Experiment=rep(i, length(this.data$IPs)),                   
                                        milliseconds=as.integer(this.data$milliseconds),
                                        IPs=as.integer(this.data$IPs),
                                        actualIPs=as.integer(this.data$actualIPs),
                                        PUTs=as.integer(this.data$PUTs),
                                        actualPUTs=as.integer(this.data$actualPUTs),
                                        PUTsIP=as.integer(this.data$PUTs)/as.integer(this.data$IPs)))

    this.data <- this.data[this.data$actualPUTs>=16,]
    experiment.aggregates.filtered <- rbind( experiment.aggregates.filtered,
                                            data.frame( Experiment=i,                   
                                                       milliseconds=median(as.integer(this.data$milliseconds)),
                                                       IPs=median(as.integer(this.data$IPs)),
                                                       actualIPs=median(as.integer(this.data$actualIPs)),
                                                       PUTs=median(as.integer(this.data$PUTs)),
                                                       actualPUTs=median(as.integer(this.data$actualPUTs)),
                                                       count= length(this.data$IPs),
                                                       PUTsIP=median(as.integer(this.data$PUTs)/as.integer(this.data$IPs))
                                                       )
                                            )

}

experiment.data$newgraph <- grepl("newgraph",experiment.data$Experiment)
experiment.data$nooverlap <- grepl("nooverlap",experiment.data$Experiment)
experiment.data$reboot <- grepl("reboot",experiment.data$Experiment)
experiment.data$cachecrash <- grepl("cache",experiment.data$Experiment)

time.data <- data.frame(Experiment=character(),
                        minute=as.Date(character()),
                        IPs=character())

these.experiments <- c("nooverlap-0","nooverlap-newgraph","nooverlap-newgraph-reboot","nooverlap-newgraph-reboot-cachecrash")
ip.correlations <- data.frame(Experiment=character(),
                              correlation=character())

for ( i in these.experiments ) {
    this.data <- read.csv(paste0("../data/2016-PPSN/ips-per-minute-cache=32-",i,".csv"))
    this.data.plus1 <- this.data[2:length(this.data$IPs),]
    ip.correlations <- rbind(ip.correlations, 
                             data.frame(Experiment=i,
                                        correlation = cor(this.data[c(1:length(this.data$IPs)-1),]$IPs
                                                         ,this.data.plus1$IPs)))
    time.data <- rbind(time.data,
                       data.frame(Experiment=rep(i,length(this.data$IPs)),
                                  minute=strptime(this.data$time,"%Y-%m-%d %H:%M"),
                                  IPs=as.integer(this.data$IPs))
                       )
}

# New data for this paper

IPs <- read.csv("../data/nodio-2016-3-17-0_per_minute.csv")
IPs$minute <-  as.POSIXct(IPs$time, format="%Y-%m-%dT%H:%M")


minutes.data <- data.frame(Experiment="17-0",                      
                           minute=IPs$minute,
                           IPs=IPs$IPs,
                           PUTs=IPs$PUTs,
                           updates=IPs$updates,
                           ratio=IPs$updates/IPs$PUTs)


@

\title{A performance assessment of evolutionary algorithms in
  volunteer computing environments} 


% \author{Juan-J.~Merelo \and Paloma de las Cuevas \and Pablo
%   Garc\'ia-S\'anchez \inst{1} \and Mario Garc\'ia-Valdez \inst{2}}

% \institute{Dept. of Computer Architecture and Technology, University
% of Granada, Spain \and
% Dept. of Graduate Studies at Instituto Tecnol\'ogico de Tijuana}

\author{A.U. Thor \and A. N. Otherauthor \and Just A. Nother \inst{1}
  \and Jet A. Nother \inst{2}}

\institute{Invisible University, Frantzia \and
Institute of Higher Studies, Zootropa}


\maketitle

%\thispagestyle{empty}
%\pagestyle{empty}

\begin{abstract}
In a volunteer distributed computing system, users run a program on
their own machine to contribute to a common effort. If the program is
embedded in a web page, collaboration is easy, but also ephemeral. In
this paper, we analyze a volunteer evolutionary computing system called
NodIO, trying to find out a few rules that encourage volunteer
participation and how to make the most of their contributions to the
evolutionary algorithm. We will show different measures of
participation and contribution to the algorithm and how different
volunteer usage patterns and tweaks in the algorithm, such as restarting clients when solution has been found, contribute to
improvements and leveraging of these contributions. 
\end{abstract}

\section{Introduction}
\label{introduction}


The world wide web provides not only a platform for content
distribution, but also an increasingly reliable and high-performance
operating system for running distributed applications. This is based
in two key things: the Javascript virtual machine every browser runs
and a simplified standard interface for interacting with servers.
Thus creating a distributed computing experiment is just a matter of
making a JavaScript application interchange information with a server,
called REST. From the point of view of the programmer, this involves
relatively common skills and no special libraries, since that
interface is built in the browser, and a simple application that
responds to those requests on the server side; both involve just a few
dozens lines of code additionally to whatever business logic your
application has. But, more importantly and from the point of view of
the user, that application can be run by simply visiting a web page.

Using this system for creating distributed experiments is called {\em
  volunteer}, {\em cycle-scavenging} or {\em opportunistic} computing
\cite{sarmenta2001volunteer} and it dates back, in different shapes
and underlying mechanisms, to the origin of the web \cite{david-seti:home}. Our interest
here, however, is to use it as a resource for evolutionary
computation, as our group has done for a long time \cite{jj-ppsn98}.

In this line of research that uses volunteer computing for
evolutionary algorithms, there are several pending issues. The first
and maybe most important is approaching volunteer computing as a
techno-social system \cite{vespignani2009predicting} which integrates
user decisions and behavioral patterns in the system model; this
includes trying to optimize the number of users in a particular
experiment. The second line of research, although related to the
first, is more focused on the evolutionary algorithm itself and how
different design decisions will affect its performance.
% Is it ok that we say that the first line of research is the most important... but then we focus on the second?
We have approached the first issue in our previous work,
% I have justified to focus, in this case, on the second
but in this paper our focus will be in the second aspect: we will try to
design a decentralized system that, at the same time, is able to use
all available resources for finding the solution of an evolutionary algorithm. This design will be done incrementally by
changing client and the server and measuring its
impact on the overall performance: time and evaluations needed to find
the solution. Eventually, we want to find a system that, whatever the
number of users available to perform the experiment, is able to
maximize their contribution to the evolutionary algorithm, at the same
time that the evolutionary algorithm itself makes the most of those
contributions and is able to find
the solution to the problem in a minimum time and with the least
number of contributions.

The rest of the paper is organized as follows: Next we will briefly
present the state of the art in opportunistic distributed evolutionary
computation (EC). Section \ref{sec:description} will describe the
framework and problem used in the experiments, which are publicly
available under a free license. We will present the results of the
different steps in the incremental design in Section
\ref{sec:experiments}, to finally wrap up with the conclusions. 

%---------------------------------------------------------------
\section{State of the art}
\label{sec:soa}

Volunteer computing involves a user running a program voluntarily
and, as such, has been deployed in many different ways from the
beginning of the Internet, starting with the SETI@home framework for
processing extraterrestrial signals \cite{david-seti:home}. However
the dual facts of the introduction of JavaScript as a universal language for the
browser and the browser itself as an ubiquitous web and Internet client has
made this combination the most popular for volunteer computing
frameworks such as the one we are using here, and whose first version
was described in \cite{DBLP:conf/gecco/GuervosG15}.

JavaScript can be used for either unwitting
\cite{unwitting-ec,boldrin2007distributed} or volunteer 
\cite{langdon:2005:metas,gecco07:workshop:dcor} distributed
evolutionary computation and it has been used ever since by several
authors, including more recent efforts
\cite{duda2013distributed,DBLP:journals/corr/abs-0801-1210,EvoStar2014:jsEO}. 

In general, those papers go no further than a proof of concept,
attempting to gauge how many users join the effort and also how long
would it take for a concrete amount of users to reach the solution. In
general, they do not try to assess the performance of the algorithm
itself in the precise conditions they find in a volunteer computing
system, although papers such as the one by Laredo et
al. \cite{churn08:ijhpsa}, using models, try to find out the resulting
performance when the users are changing continuously according to a
Weibull distribution. In the case of Klein and Spector \cite{unwitting-ec}, the algorithm
is actually run on the server, using the browser mainly for fitness
evaluation so no actual contribution to the evaluation of the
evolutionary algorithm itself is made.

There is another factor that should be taken into account: since the
user has control of the browser, there is a limited amount of
interaction with it, namely, the fact that by reloading the webpage they
can apply a kind of hypermutation, killing the current population and
generating new individuals some of which will make their way to the
common pool via migration. In that sense, volunteer computing is also
a way of {\em human computation} \cite{quinn2011human}, a concept that
has also been applied to evolutionary algorithms \cite{972056}, in
this case extensively and with all operators. It is quite difficult to
find out how many times this happens and what is their effect on the
overall algorithm, but this only reaffirms the fact that there is a
long list of issues with volunteer evolutionary computation, and that
the volunteer him or herself is at the center of many of them.

In this paper we will make contributions towards the design of
efficient volunteer evolutionary algorithms by studying the effect of
several design decisions on performance. In the next Section we will
present the general framework and the initial setup.


\section{Description of the framework}
\label{sec:description}

In general, a distributed volunteer-based evolutionary computation
system based on the browser is simply a client-server system
which client is embedded in the browser via JavaScript. We call this
system {\sf NodIO}. All parts of the framework are free and available
with a free license from \url{https://ANONYMOUS-URL}.
%\url{https://github.com/JJ/splash-volunteer}. 

The architecture of the {\sf NodIO} system is divided in two tiers:\begin{enumerate}
\item A REST (representational state transfer) server that responds to
  HTTP requests made by the browsers. These HTTP requests use JSON for information 
  encoding, in this case chromosomes and related
  information sent by the clients and responses returned by the
  server.  The server has the capability to
  run a single experiment, storing the incoming chromosomes in a
  key-value cache that uses as key the chromosome string itself and is
  reset when the solution is found. This cache has a finite 
  size that erases the oldest chromosomes once it has been filled to % Also "it has been filled to its"
  capacity.  
\item A client that includes the evolutionary algorithm as
  JavaScript code embedded in a web page that displays the fitness and
  other graphs, some
  additional links, and information on the experiment. This code runs
  an evolutionary algorithm {\em island} that starts with a random
  population, then after every 100 generations, it sends the best individual
  back to the server (via a {\tt PUT} request), and then requests a random
  individual back from the server (via a {\tt GET} request). We have
  kept the number of generations between migrations fixed since it is
  a way of finding out how much real work every client is doing. In
  the version of the server used in this paper, we implemented a
  policy by which the combination IP-fitness chromosome could only be
  inserted once into the cache. That means that if a particular IP
  sends the fitness 99 into the server, it will be accepted only
  once. The main intention of this policy was to avoid {\em
    overlapping} of contributions, with clients still running an {\em
    old} run of the experiment contributing after a new one had
  started. This did not completely avoid these contributions: if the
  clients of the {\em old} run increased their new fitness, it would
  still be allowed. 
  % As I remember we kept the run_number so IPs with old runs
  % did not interfere with newer ones. But you are saying: what if they are super slow 
  % and always PUT when the run is already over?, we want to receive 
  % those individuals? 
  % "That means that if a particular IP
  % sends the fitness 99 into the server, it will be accepted only
  % once."  
  % once per run?
% JJ - once per run or experiment. But we didn't keep that run_number
% to avoid interference, just to keep track
  %  but different individuals could have the same fitness, are we limiting the search? 
% We are actually checking for the same individual, not only the fitness
  %  maybe we could have multiple versions of the pool a la MVCC  :)
% Let's leave that for the future...- JJ
  % I think if we are just trying to limit overlapping of the experiments
  % then PUTs should send the run's seq_number and completly remove it.  
% The client does not keep track of the seq number. Maybe we should do
% just that, but sending back and forth the seq number implies
% refactoring. - JJ
\end{enumerate}

Figure \ref{fig:system} describes the general system architecture and
algorithm behavior. Different JavaScript libraries, such as JQuery or {\tt
  Chart.js} have been used to build the user interface elements of the
framework, which should be running in \url{http://ANONYMOUS-URL}, 
%\url{http://nodio-jmerelo.rhcloud.com}, 
a free resource hosted in the
OpenShift Platform as a Service\footnote{It is not guaranteed to be running,
or running the same version, when you read this, however; you can always get
the sources from GitHub and set it up yourself.}.
%
\begin{figure}[!t]
\centering
\includegraphics[width=3in]{system.pdf}
\caption{Description of the proposed system. Clients execute a JavaScript EA
  in the browser, which, every 100 generations, sends the best
  individual and receives a random one back from the server.}
\label{fig:system}
\end{figure}

NodIO needs a fitness function to work with; in this case the classical Trap 
function \cite{Ackley1987} has been used. 
The evolutionary algorithm that uses Trap as fitness function is
completely run in the
browser, but the server needs to know when the solution has been
found, so a subroutine that checks if the solution been found  is also included in the server, as a configuration
option. In all cases we are using 50 Traps, a problem
that is difficult enough to need the intervention of several users for
an extended amount of minutes.
For every new individual sent by the clients to the cache, several
pieces of metadata were stored: a time stamp, the client IP, the chromosome and
fitness value, the cache size in that particular moment, and also if that
individual actually updated the cache or not. 

All experiments are announced in the same way: we use social networks,
mainly Twitter, to announce the start of a new experiment. LinkedIn,
Facebook, and also private groups in Telegram and WhatsApp have been
used for announcements too. We also encourage friends and followers to
put an original post for announcing it, instead of just sharing, in
the way allowed by the platform, the original post. During the running
of the experiments, users are engaged, questions answered, and all
kind of explanations given if requested. In general, announcements
have been made in Spanish, but in some cases English was used
too. This makes for a certainly unreliable experimentation framework,
but it is a realistic one. The steps in the design of the evolutionary
algorithm will be explained next.

%---------------------------------------------------------------
\section{Experiments and results} 
\label{sec:experiments}

\begin{table}[htb]
\caption{Summary of results for the 5 sets of experiments.\label{tab:runs}}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
Experiment & Median Time & \multicolumn{2}{|c|}{Median IPs} &
\multicolumn{3}{|c|}{Median PUTs} & \#Runs & Unique IPs & Puts/IP \\
           &   (ms)        & Total & Used & Total & Used & Ratio &  & & \\
\hline
base & 120056.0 & 10 & 4 & 256.0 & 21.0 & 0.1290323 & 81 & 262 & 21.33333\\
\hline
graph1 & 152619.5 & 9 & 4 & 212.5 & 26.5 & 0.1282238 & 36 & 96 & 24.30000\\

graph2 & 1173561.5 & 6 & 4 & 761.5 & 22.0 & 0.0520509 & 36 & 73 & 129.04545\\
\hline
reboot & 796214.0 & 9 & 6 & 646.0 & 67.0 & 0.0955121 & 45 & 168 & 75.88889\\
\hline
cachereboot & 392917.0 & 12 & 12 & 576.0 & 79.0 & 0.1575092 & 55 & 225 & 43.28571\\
\hline
\end{tabular}
%% \end{tabular}
%% Initially generated with this code, 
% <<experiments, cache=FALSE,echo=FALSE>>=
%experiment.aggregates$Experiment <- c("base","graph1","graph2","reboot","cachereboot")
%kable(experiment.aggregates)
%@ %def 
\end{center}
\end{table}
%
%
%
Since the point of using volunteer computing system is to cut costs,
all experiments were set up using the OpenShift
PaaS, which provides a free tier, making the whole experiment cost
equal to \$0.00. In fact, the NodIO system can be deployed to any
system as long as it can run node.js and has access to the filesystem, for
log files, if available%% . All these logs are available upon
%% request. Processed files are available in the same repository that
%% hosts this paper, as well as the scripts needed to process them. 

%
%
\begin{figure}[htbp]
\centering
<<zipf, cache=FALSE,echo=FALSE,fig.height=4>>=
ggplot(ips.data,aes(x=idx,y=PUTs,group=Experiment,color=Experiment))+geom_point()+scale_y_log10()+ scale_colour_discrete(labels=c('base','graph1','graph2','reboot','cachereboot'))+scale_x_log10()
@ %def graph for IPs and puts
%
\caption{Log-log plot of ranked and normalized number of contributions (PUTs) per IP.\label{fig:zipf}}
\end{figure}
%
Every run had slightly different conditions, although in some cases it
was just a server reboot and a new round of announcements. Each run
batch included at least 30 completions, that is, evolution until the
solution was found. The number of runs was controlled by polling an
URL that indicated the number of runs made in the present batch so
far. Every volunteer contributed a certain number of individuals
(represented by the PUT HTTP request) to the server. The ranked number
of contributions per unique IP is shown in Figure \ref{fig:zipf} in a
log-log scale. Besides following the usual Zipf's law found in our
previous work and thus having a very similar appearance for all 5 sets, it
is interesting to see how some sets have a number of contributions per
IP that are slightly superior to the rest, specially so in {\tt
  graph2}. The last two sets, {\tt reboot} and {\tt cachereboot}, also
show a regime in the middle of the graph that shows that some users
are contributing more than in other similar sets. In general, these
differences in behavior are due more to the specific users that show
up in these experiments than to general laws, but it should also be
noted that those behaviors can be encouraged by design.

A summary of the experiments and  their results is shown in Table
\ref{tab:runs}. This table shows the median time needed to find the
solution as well as the median number of IPs and PUTs. In this case we
distinguish between the total number of clients participating in every
particular run, and the {\em actual} number whose contributions were
accepted into the cache, which was set up to accept those only if {\em
  new} chromosomes were sent to the server in order to avoid overlaps
between one run and the next. That is why, in both cases, the
``Contributing'' column includes a value that is less than the total
number. The next column includes the total number of runs in that
particular batch, finished roughly after a minimum number of 30 was reached. 

The baseline, called {\em base} in the Table, uses the
mechanism of dropping contributions from clients if they are repeated
to avoid overlap. Clients still get a random chromosome from the
server, so they can in fact proceed with the algorithm and even finish
it. However, this was not known to the volunteer, so the fact that they were not contributing to the pool was
conveyed by inserting a graph in the client that represented the size
of the cache, labeled ``How am I contributing''. The user could
then realize there was no contribution and do something
about it. These sets of runs are labeled {\tt graph1} and 2. However, a stuck
client could still add to the pool if it found a new optimum, so we
added a mechanism for rebooting the client, that is, reloading the
page, if the cache size was found to be less than 1, which indicated a
run recently started. This batch of runs is labelled {\tt
  reboot}. This size of the cache was not really noticed by some slow clients or too fast runs, so that in the
last case, labeled {\tt cachecrash}, clients rebooted if the cache
collapsed by more than a certain length, indicating a cache that had
been voided and was filling again. Our objective was twofold: to
encourage engagement by the users and also to increase the involvement
of every client in the common experiment, eliminating at the same time
overlaps between runs. 

%
\begin{table}
  \caption{Experiments with {\em overlapped} runs filtered out: summary of results. \label{tab:filtered}}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
Experiment & Median Time & \multicolumn{2}{|c|}{Median IPs} &\multicolumn{2}{|c|}{Median PUTs} & \#Runs &  Puts/IP \\
&   (ms)        & Total & Used & Total & Used &  & \\
\hline
base & 293685 & 13.0 & 8.0 & 483.0 & 57.0 & 47 & 35.50000\\
\hline
graph1 & 768053 & 12.0 & 5.0 & 601.0 & 79.0 & 21 & 45.84615\\
graph2 & 1477969 & 6.0 & 4.0 & 901.0 & 52.0 & 25 & 149.42857\\
\hline
reboot & 799471 & 9.5 & 6.5 & 656.5 & 67.0 & 44 & 76.48016\\
\hline
cachereboot & 463347 & 12.0 & 12.0 & 638.5 & 80.5 & 52 & 44.25911\\
\hline
\end{tabular}
%% Initially generated with this code, 
%%<<experiments.filtered, cache=FALSE,echo=FALSE>>=
%%experiment.aggregates.filtered$Experiment <- c("base","graph1","graph2","reboot","cachereboot")
%%kable(experiment.aggregates.filtered)
%%@ %def 
%
\end{center}
\end{table}
%
The first thin we will have to check is whether we have effectively eliminated these
overlaps. In order to do that, we will have to establish a threshold
under which we can reasonably expect that the solution cannot be found. In every
evolutionary algorithm it might happen, by chance, that the solution
is found in just a few evaluations, but that is usually not the
case. By looking at the logs in the last case we have established that
minimum as 16 PUTs; in less than 1600 generations no solution to that
difficult problem will be found. Let us filter then the runs,
eliminating those that used less than 1600 generations to find the
solution which can reasonably be said that they did so because some
browser ``carried over'' a chromosome from the previous run. The
results are shown in Table \ref{tab:filtered}. The first change is in
the column \#Runs, that shows that most of the runs in the initial
configurations found the solution thanks to these carried over
clients. However, in the last two rows, the ones that rebooted clients
when they detected changes in cache, the difference is minimal. This
means that one objective has been reached: if clients reboot when they
detect changes in the cache that imply a new run, overlaps are almost
eliminated. Going back to Table \ref{tab:runs}, we can see than the
ratio of effective contributions, those that feed the cache, increases
from around 13\% in the first case to almost 16\% in the last,
reaching lows of less than 6\% in some cases ({\tt graph2}). This
indicates that rebooting the clients has the positive effect of
increasing the contribution of clients to the common pool. However, we
should remark that the clients will still stop contributing if they
get stuck in a particular solution, so the ratio keeps being low
resulting in a slow change in the cache and a lower diversity of
clients that tap that cache. At any rate, an increased number of
effective PUTs will have a positive impact on the algorithm.

\begin{table}[htbp]
  \caption{Minute-wise correlation for number of participating volunteers. \label{tab:correlation}}
\begin{center}
<<correlations, cache=FALSE,echo=FALSE>>=
ip.correlations$Experiment <- c("base","graph1","reboot","cachereboot")
kable(ip.correlations)
@ %def correlations
\end{center}
\end{table}
%
We can check this impact by looking at the column that refers to
median time in Table \ref{tab:filtered} and its relation to the median
number of PUTs, which reflects the number of evaluations performed by
the clients. The last two rows have a low number of evaluations,
around 650, which is higher than the first two, probably due to the
lower volunteer supply, but better than the middle one; besides, the
amount is remarkably similar. But in the last case, it takes much less
time to find the solution {\em using the same number of
  evaluations}; the same situation applies if we compare {\tt graph1}
to {\tt cachereboot}. All variables are difficult to control in this
environment, but it might be due to the fact that parallelism has
increased, there are more volunteers contributing at the same time. In
fact, as it can be seen in the last column, the median number of
contributions per user is the lowest for {\tt cachereboot}, which so needs less contributions by users to
find the same solution. Besides, the last row shows that {\em all}
users actively contribute to the cache, while in the {\tt reboot}
strategy 30\% of users do not. This might be due to the fact that {\tt
  reboot} only does so when cache size is less than one, indicating
that this strategy is only partly successful in avoiding overlaps.  

In fact, Table \ref{tab:correlation}, which shows
the correlation between the number of IPs from one minute to the
next, presents a similar  behavior for all experiments but the last one. A high correlation indicating that the number of volunteers changes
slowly from one minute to the next; however, the {\tt cachereboot} experiment correlation indicates a bigger
dynamism, with volunteers joining and leaving the experiment all the
time, maybe because experiments take less time and need less resources.

\section{Finding the keys to a good performance}
%
%
\begin{figure}[ht!b]
\centering
<<minutedata, cache=FALSE,echo=FALSE,fig.height=6>>=
ggplot(data=minutes.data, aes(x=minute,y=PUTs,color=ratio))+ scale_x_datetime()+geom_point()+stat_smooth()
@ %def graph on PUTs per minute
%
\caption{Plotting the contributions per minute in a new and massive
  experiment, as well as the real number as color. Lighter color
  indicates that all PUTs contributed new individuals to the
  experiment, darker color few to none. \label{fig:minute}}
\end{figure}

%---------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

Our main intention in this paper was to configure an evolutionary
algorithm in a volunteer computing environment to improve its
efficiency. This initial configuration included
{\em overlaps} or clients running islands that had actually started in
a previous experiment in the set. If we are interested in measuring
how many users or their behavior in bulk this is not so important, but
in this case our focus was on the algorithm so we had to try that all
clients participating in it started more or less at the same time and
from a random population. In fact, we managed to do so, except in some
byzantine cases, by making the clients restart when the cache size
collapses to 1 or to a size much smaller than the current; we called
this client configuration {\tt reboot} and {\tt cachecrash}.  This
configuration achiever a positive secondary effect: all clients
contribute to fill the cache, having thus an actual effect on the
chromosome pool and effectively making all clients present contribute
to the simulation. 

The last configuration managed to find, with a relatively small number
of evaluations, solutions in half the time that in previous sets of
experiments. Although in the {\tt base} and {\tt graph} the median
time and number of contributions was smaller, we cannot disregard the
possibility that many of those solutions were found thanks to
contributions from {\em carried over} islands with an algorithm that
has run a big amount of evaluations already. At any rate, the {\tt
  cachereboot} finds on average solutions four times as fast as {\tt
  graph2} with twice as many average volunteers (or three times as
many if we consider {\em effective} volunteers), needing 50\% less
evaluations, thus meaning a big improvement in the performance of the
evolutionary algorithm, which was the target of this paper. This
improvement is less with respect to {\tt reboot}, but in any case the
method to detect the beginning of a new run is much more effective and
will be kept in the future.

This future work holds many challenges which once again will go in two
different directions: gathering more users and improving the
efficiency of the algorithm run in an asynchronous way by the
users. In the first case, it will be interesting to make it more {\em
  social} by comparing local performance to other's contributions, and
showing it in a graph or being able to tweet advances or the fact that
the solution has been found. Anything that keeps the users running the
system and attracts new users will contribute to the speed of finding
the solution. And the algorithm will have to be improved, mainly by
making the population more diverse. Right now there is a rigid policy
of contribution to the pool: send the best individual, but this could
be relaxed. Some other policies could also be tested, as well as
running heterogeneous algorithms in the same way that was done in
\cite{DBLP:journals/grid/ValdezTGVO15}. Diversity, as in any other
evolutionary algorithm, seems to be the key, but there are many ways of
approaching it. 

All these avenues of experimentation will be done openly following the
Open Science policy of our group, which, in fact, contributes to
establish trust and security between us and volunteers and is an
essential feature of the system. That is why this paper, as well as
the data and processing scripts, are published with a free license in GitHub at
\url{ANONYMOUS-URL}.
%\url{https://git.io/gecco-es-15}.

%---------------------------------------------------------------
\section*{Acknowledgments}

% This work has been supported in part by TIN2014-56494-C4-3-P (Spanish
% Ministry of Economy and Competitivity), PROY-PP2015-06 (Plan Propio
% 2015 UGR). We are also grateful to {\tt @otisdriftwood} for his help
% gathering users for the new experiments.

Several projects\\
Taking\\
This\\
Much\\
Space\\ 

\bibliographystyle{splncs}
\bibliography{volunteer,GA-general,geneura,javascript,ror-js}

\end{document}

%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% End:
