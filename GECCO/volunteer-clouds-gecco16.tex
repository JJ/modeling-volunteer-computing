% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}

\graphicspath{{../img/}}

% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}

\usepackage{array}

% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}



\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\doi{10.475/123_4}

% ISBN
\isbn{123-4567-24-567/08/06}

%Conference
\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

\acmPrice{\$15.00}

%
% --- Author Metadata here ---
\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Performance for the masses: Experiments with A Web Based Architecture to Harness Volunteer Resources 
for Low Cost Distributed Evolutionary Computation }
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{5} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
% \author{Juan-J.~Merelo, Dept. of Computer Architecture and Technology
%   and CITIC, University of Granada, Granada, Spain \\
% Mario Garc\'ia-Valdez, Dept. of Graduate Studies, Instituto
% Tecnol√≥gico de Tijuana, Tijuana, M\'exico\\
% Pedro A. Castillo, Dept. of Computer Architecture and Technology, University of Granada, Granada, Spain\\
% Pablo Garc\'ia-S\'anchez, Dept. of Computer Architecture and Technology, University of Granada, Granada, Spain\\
% Paloma de las Cuevas, Dept. of Computer Architecture and Technology,
% University of Granada, Granada, Spain\\
% Nuria Rico, Dept. of Statistics and Operational Research, University of Granada, Granada, Spain%

% \thanks{Corresponding author: Juan-J.~Merelo (Email: jmerelo@ugr.es).}%
% }
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{1932 Wallamaloo Lane}\\
       \affaddr{Wallamaloo, New Zealand}\\
       \email{trovato@corporation.com}
% 2nd. author
\alignauthor
G.K.M. Tobin\titlenote{The secretary disavows
any knowledge of this author's actions.}\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{P.O. Box 1212}\\
       \affaddr{Dublin, Ohio 43017-6221}\\
       \email{webmaster@marysville-ohio.com}
% 3rd. author
\alignauthor Lars Th{\o}rv{\"a}ld\titlenote{This author is the
one who did all the really hard work.}\\
       \affaddr{The Th{\o}rv{\"a}ld Group}\\
       \affaddr{1 Th{\o}rv{\"a}ld Circle}\\
       \affaddr{Hekla, Iceland}\\
       \email{larst@affiliation.org}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor Lawrence P. Leipuner\\
       \affaddr{Brookhaven Laboratories}\\
       \affaddr{Brookhaven National Lab}\\
       \affaddr{P.O. Box 5000}\\
       \email{lleipuner@researchlabs.org}
% 5th. author
\alignauthor Sean Fogarty\\
       \affaddr{NASA Ames Research Center}\\
       \affaddr{Moffett Field}\\
       \affaddr{California 94035}\\
       \email{fogartys@amesres.org}
}

\maketitle





\maketitle

\begin{abstract}

Using the browser as a computing resource in volunteer computing has presented several advantages, 
but it remains a challenge to fully harness both the users' behaviour and the browser's capabilities.
That is why we have enhanced {\sf NodIO}, a framework used to create client-server architectures 
for distributed evolutionary algorithms, resulting in {\sf
  NodIO-W$^2$}, whose results we present in this paper.
{\sf NodIO-W$^2$} leverages the HTML5 Web Workers specification to execute in the background several clients per
browser, so that the the number of generations per user could be improved. The enhancements on the NodIO architecture,
along with some initial results on the expected performance and scaling behavior when the number of users is increased,
are introduced. We also use the experimental data to find out what kind of patterns the user behavior follows by examining 
the resources they devote to our experiment, and try to fit them in some statistical distribution. In general, 
our experiments prove, first, that {\sf NodIO} is a flexible and low-overhead platform to perform distributed computation
experiments that include volunteers, and then that user behavior
follows some patterns, and, thus, are amenable to a model and thus predictable.

\end{abstract}

\keywords{Volunteer computing, distributed computing, cloud computing}


%---------------------------------------------------------------
\section{Introduction}

Nowadays there is a big and increasing number of
distributed evolutionary computation software frameworks available on
the market, usually as open source frameworks \cite{Parejo12Survey}. 
Most of these frameworks have 
two features: they are {\em desktop} applications, that is, they must
be compiled or run directly on the operating system layer and, second,
they assume that the nodes that are going to run the system are under
the control of developers or they at least have access to them. In
fact, these features are commonly found in most soft computing
frameworks \cite{7094263}. 

%Mario: Is NodIO presented here after the cut? 
%TO DO: Add reference
There is, then, space for frameworks that do not make any of these
assumptions, such as {\sf NodIO} presented in \cite{2016arXiv160101607M}. {\sf NodIO} is basically 
%assumptions, such as the one presented in this paper, which will be
% named {\sf NodIO}. {\sf NodIO} is basically
% (Paloma) Is the "first" word really needed? 
% (Mario) I guess is more related to fundamentally or primarily but "basically" sounds better
a two tier client/server 
application, in which clients can run on a browser. 
Therefore, they can be embedded in any web-enabled device, including many over which 
developers have no control.

Some time ago, our group faced the problem of diminishing funds for
buying new hardware. This was aggravated by the increasing maintenance costs and
extended downtime resulting from the continuous failures of existing
clusters.
Considering this, we leveraged our experience in the design of web
applications since their inception, including JavaScript, and other
volunteer and
unconventional distributed evolutionary computing systems to 
design and release a new free framework that would allow anyone to
create a volunteer distributed evolutionary computation (EC) experiment using cloud resources as
servers, and browsers as clients.
% (Paloma) I think you should rewrite this whole sentence.
% For starters, focus in that you "leveraged" the problem through your deeply knowledge of... this and that. And "Thus/Then/Therefore", you decided... 
% (Mario) We leverage our experience in blah blah to design
% (Paloma) It may be... guess I was thinking in the noun, not in the
% transitive verb
% Please check if there's a problem here too.

%PABLO: He a√±adido el siguiente p√°rrafo para el issue #31
There exist other volunteer computing frameworks written in compiled languages,
such as C++ or Java, that, in principle, can achieve better performance than
JavaScript \cite{2015arXiv151101088M}. However, one of the advantages of our architecture is that
clients do not need to install anything in their machines, they just
need to open a web page  
% Somewhat redundant 
% Javascript is the only language that can be executed in this way without any installation, 
% also removed the default, redundant with next sentence
in a compatible browser, such as Chrome, Safari or Firefox, which are
available in almost every desktop or mobile device. Other advantage is
that the source code can be directly examined by volunteers, because it is not
compiled as other options such as Java Applets, ActionScript or
ActiveX.
Also, as we will discuss in this paper, it can provide some extra
features such as enhanced security or operating system independence. 

%PABLO: hay que enlazar con el siguiente p√°rrafo
%Desktop clients have the same issues as any 
%volunteer computing system, including our % (Paloma) their?
% own: these issues are
%related to security as well as
%safety, which introduce 

When developing volunteer based frameworks, designers must
also consider the whole {\em social} aspect in the design, with
issues related to security, trust, and privacy among others. 
The computing system becomes then a {\em techno-social system} \cite{vespignani2009predicting}.  
% (Paloma) Is the social aspect related to the word "safety"? It's not completely clear.
%a {\em social} aspect to the design of a
%computing system that makes it a {\em techno-social system}
In this paper, our intention is to
analyze the {\sf NodIO} framework from two different
%Mario: Edit this?
%present the {\sf NodIO} framework and analyze it from two different
angles: the purely technical angle, including the decisions that went
into its features, and the social angle, to account on how the persons
that voluntarily choose to enter the web page running the experiment
determine its performance and how it can be modeled and, eventually,
predicted.

The rest of the paper is organized as follows: Next we present the
state of the art (Section \ref{sec:soa}) in web-based distributed
computational systems along with attempts to predict and model its
behavior. The first results in a volunteer setting are described
in Section \ref{sec:exp1}, and experiments using a new client
architecture that uses web workers in Section \ref{sec:w2} and then
applied to a different problem in Section \ref{sec:rastrigin}. 
Finally, conclusions and future lines of work are presented in Section
\ref{sec:conclusion}. 

%---------------------------------------------------------------
\section{State of the art}
\label{sec:soa}

The web used as a resource for distributed computing has a
long history, and almost since the beginning it was used for
evolutionary computation even before JavaScript was
introduced as a browser-based scripting language in 1997. Before that
time other technologies,
 including Flash animations and VBScript, ActiveX or Java applets were
used; for instance, Java was pointed out in \cite{soares1998get} as a
``language for the internet'', providing some advantages such as multi-architecture compatibility or
security mechanisms, and was proposed in \cite{chandy1996world} as the
basis for the creation of a world-wide peer to peer system using compiled Java
programs. The ATLAS system
\cite{Baldeschwieler:1996:TIG:504450.504482} also used Java together
with another language called Cilk; all these papers indicated that
the introduction of an architecture-independent system such as the
Java virtual machine opened the doors for ultrascale, Internet-wide,
metacomputers. 
%% We need to only reference this section?:
%%------------------- BEGIN
% This paper % (Paloma) No really need for that "also", plus it complicates reading
% establishes the desirable features of
% any {\em global computing} infrastructure, that can also be applied to
% distributed evolutionary computation experiments that rely on
% volunteer computing: scalability, heterogeneity, fault tolerance,
% adaptive parallelism, volunteer's safety, anonymity, hierarchy, ease of use, and
% reasonable performance. But hierarchy, this is, the capability of
% preferentially using resources in the same organization, is a feature
% that could be either disregarded or included within adaptivity, which
% leaves us with eight desirable properties. 
% % (Paloma) Re-write. For example, "But is hierarchy, this is, the
% % capability of preferentially using resources in the same
% % organization, the one that could be either disregarded or included
% % in adaptivity, which leaves us with eight desirable properties." 
% % (Paloma) And I re-wrote what I proposed yesterday.
% % thanks :-) -JJ
% %%------------------- END
% %% EDIT:
% %%------------------- BEGIN
% The fulfillment of these properties could be achieved by using any of the
% languages and associated technologies mentioned above, provided that the user had
% installed the required virtual machine plugin first. 
% %%------------------- END
The fulfillment of the desirable features of any distributed evolutionary 
computation experiments that rely on volunteer computing \cite{2016arXiv160101607M} could be achieved by using any of the
languages and associated technologies, mentioned above, provided that the user had
installed the required virtual machine plugin first. 

% (Paloma) These should be separate sentences
However, embedding a
distributed computation client in the browser has the advantage of
providing a single and universal way of accessing resources via an
URI, something that is not guaranteed by using desktop applications. 
Baratloo et al. tried in 1996 to create a framework with
the aforementioned qualities called {\em Charlotte}
\cite{baratloo1999charlotte}. This framework, which implemented a
distributed shared memory, was inspired by a previous work
on parallel computing libraries PVM and MPI and solved most of the issues by
using the Java virtual machine embedded in the browser,
which provided
security to the user, uniformity of computing environment
independently of the operating system, and access to a common data store in the
server. Additionally, since the Java virtual machine is a sandbox, it was safe to the user
and as it runs compiled bytecode it also guaranteed a certain level of
performance. % (Paloma) I've changed a bit this sentence to unify the use of past tense when talking about Charlotte
Other issues such as scheduling, which are related to adaptive 
parallelism, were solved on the server. It is interesting to note that,
even in the early stages of the web, the possibility of
% (Paloma) Re-write. For example, "but it is interesting to note that, even in the early stages of the web, the possibility of..."
browser-based distributed computing was already researched. 


Other systems, like JET, proposed by Soares et
al. \cite{soares1998get}, also used Java. JET supports
the execution of parallel applications over the Internet and features
a comprehensive statistics collection subsystem, enabling its use for science, %(Mario) Was easier to change "allowing"
since it allows % (Paloma) Use synonyms, like "yield" or "permit" 
the collection of algorithm analytics by the
experiment designer, unlike the other systems mentioned
above. Other proposals contemporary to JET, such as {\em SuperWeb}
\cite{alexandrov1997superweb} are roughly a translation of classical
distribution techniques, with overly complicated architectures that include
brokers for resource discovery. SuperWeb is focused,
% (Paloma) Careful with starting using plural ("Other proposals", "are") and continuig with singular. If you are referring to "SuperWeb", change "It" for "SuperWeb"
however, in solving the
issues involved in browser-based computing and also in making a system
that is able to work across different types of computers, mentioning
SPARCstations and PCs with Windows-95. Interestingly enough, this
paper incorporates a discussion on the economic model, paving the way
for the for-profit distributed computing systems that arose later on. 
On the other hand, the potential for volunteer computing using
browsers was realized
later on \cite{sarmenta-bayanihan} as well as the potential of
user misconduct when having access to the source code \cite{sarmenta-sabotagetolerance}. 
However, these early efforts by
Sarmenta once again used Java and not JavaScript, making them % (Paloma) making them
less widespread.

The main problem with Java and all the other browser-embedded
technologies is that they are not {\em universal} in the
sense that an extra component, namely, the Java virtual machine or another
virtual environment has to be installed in the browser.
Also these virtual environments have become targets of malicious attacks; 
for instance numerous vulnerabilities have been discovered in 
the proprietary Adobe Flash runtime environment and
video player \cite{ford2009analyzing,watanabe2010new}. The need for
these additional environments has been mitigated with the
adoption of the HTML5 standard \cite{anthes2012html5}.
JavaScript \cite{flanagan2006javascript}, however, has become a set of standards
\cite{ECMA-262} that include the language and its components. Since
early on, it was adopted by the industry and also by scientists,
who used them for creating an evolutionary algorithm (EA from now on) on
the browser as early as 1998 \cite{jj-ppsn98}.

JavaScript can be used either for unwitting
\cite{unwitting-ec} or volunteer 
\cite{langdon:2005:metas,gecco07:workshop:dcor} distributed
evolutionary computation and it has been used ever since by several
authors, including more recent efforts
\cite{Desell:2008:AHG:1389095.1389273,duda2013distributed,DBLP:journals/corr/abs-0801-1210} 
that even
used the client's GPU \cite{duda2013gpu} or create an object-oriented
framework for evolution in the browser \cite{EvoStar2014:jsEO}. Many other researchers have
used Java \cite{chong:1999:jDGPi} and others have gone away from the
server-based paradigm to embrace peer to peer systems
\cite{jin2006constructing,10.1109/ICICSE.2008.99,DBLP:conf/3pgcic/GuervosMFEL12}. These computing
platforms avoid single points of failure (the server) but, since they
need a certain amount of infrastructure installed to start, the
threshold to join them is much lower. % (Paloma) Do yo mean... higher? A low threshold means that is easy to happen something.

Recent works propose the use of cloud computing services for the components of
client-server distributed EAs. Cloud resources
can reduce operational costs by outsourcing both hardware and software maintenance
to the cloud provider. Another advantage is that they enable the provisioning of computing resources beyond what
is available in most laboratories, allowing researchers to
scale algorithms at reasonable costs. Researchers have proposed the use of
public clouds such as Amazon EC2 \cite{CloudScale}, Google App Engine\cite{di2013towards},
and DropBox~\cite{mericloud}. The use of JavaScript to implement
server side components
gives software developers certain advantages in the cloud
paradigm, since it is a first-class citizen
in most Platforms as a Service (PaaS) products as the {\tt Node.js} framework is supported by most of them \cite{wood13:nodejs:paas}. Many web services
have native JavaScript libraries too, and also use the JSON (JavaScript Object Notation) format for data interchange. Another advantage is that
developers can use the same software tools for both client and server
side development.

JavaScript is also compatible with the functional programming paradigm \cite{Cousineau1998,MacLennan1990,Thompson1996}.
According to \cite{swanresearch2015}, the benefit of using this kind of paradigm in
metaheuristics optimization is that it allows an improvement in communicability,
reproductibility, interoperability, automated assembly, knowledge engineering,
and efficiency.

In the work presented in this paper, JavaScript has been used throughout the full
stack, which has the advantage of using the self-same lines of code for
writing an evolutionary algorithm for the
desktop or command line and browser clients, both based in the EC
library {\sf NodEO} \cite{DBLP:conf/gecco/GuervosVGES14}. This saves time 
when writing code and leverages 
the expertise acquired
% (Paloma) Not sure if "acquired" should be written before "expertise"... sounds better to me, anyways
 in the library itself. 

On the other hand, we are also interested in measuring the performance
of volunteer computing systems, an area in which there have been
relatively few efforts.
There were some initial attempts to avoid the differences in performance
that could be obtained from volunteers  by making
the algorithm adaptive to the kind of resources allotted to it
\cite{milani2004online}. This is actually not such a big problem in
algorithms such as the EA that can easily be
parallelized via population splitting or by farming out evaluations to all
available nodes. Lately, several approaches have focused on the
fault-tolerance of volunteer algorithms
\cite{gonzalez2010characterizing} which it can, of course, be studied in
the more general context of distributed computing 
\cite{nogueras2015studying} or be included in a more general study of the
performance of the EA itself
\cite{DBLP:journals/gpem/LaredoBGVAGF14}.

On the other hand, measuring the performance of the resulting metacomputer
involves understanding the dynamics of this kind of systems. Initial
work was done for peer to peer systems by Stutzbach et
al. \cite{stutzbach2006understanding} and extended to volunteer
computing by Laredo et al. \cite{churn08,laredo2008rcp}. 
%But the raw material of           
%volunteer computing, % (Paloma) Whith regard to?  Maybe it was colloquial/analogy
But some of the essential metrics in volunteer computing like the
number of users or the time spent by every one in the
computation in browser-based volunteer computing experiments, have
only been studied in a limited way in 
\cite{DBLP:journals/gpem/LaredoBGVAGF14} on the basis of a single
run. Studies using volunteer computing platforms such as SETI@home
\cite{javadi2009mining} found out that the Weibull, log-normal, and
Gamma distribution 
modeled quite well the availability of resources in several clusters
of that framework; the shape of those distributions is a skewed bell
with more resources in the {\em low} areas than in the high areas:
there are many users that give a small amount of cycles, while there
are just a few that give many cycles. This is in concordance with the
results obtained in \cite{agajaj}. Although SETI@home works
at a larger scale in both quantity of volunteers and the scope of projects, 
our initial tests indicate that
this kind of distribution might be the most appropriate to describe
user participation in several different metrics. However, in this
paper we will perform different experiments in different setups and
will try to come up with a more precise model. Next we describe the
framework itself.


As far as we know, this paper presents one of the few experiments using % (Paloma) Change to "which" in order to not repeating "that" all the time
computational resources that are as dissimilar as smartphones and
powerful laptops 
or workstation computers in a research center. 
%The methodology used
% (Paloma) I would say "the used methodology"
The algorithms used, as well as the methodology 
for gathering resources and baseline results that we will use for
comparison have been described in
\cite{2016arXiv160101607M}. We will focus next on the experimental
results using the first version of the framework. 


\section{Experimental results}
\label{sec:exp1}

Initial results using the same code base and a single computer have
been described in \cite{2016arXiv160101607M}. They prove that a bigger population
contributes to diversity and thus speeds up the solution
\cite{DBLP:conf/lion/LaredoDFGB13}. The volunteer computing 
experiments that we will describe next do not and can not have the
same conditions, but
the baseline is that if they eventually take longer than a basic
desktop, their interest will be purely academic, although they can
complement them. We will try and
prove next that that target performance can be achieved by carrying
out several experiments in different conditions. Second, they also act
as a basic performance test for the server, and also prove that the
basic server can be used for many kinds of different setups, as long
as they follow the REST protocol to deposit and obtain individuals
from the pool. It is obvious that, in this case, the server is not
really needed, since there is a single client and no interaction
between them, but there is a cost in sending and obtaining chromosomes
from the server so it is included for the sake of fairer comparison.

Initial experiments were set up using the OpenShift
PaaS, which provides a free tier within which these runs could be performed. That way, and as required, the hardware
and cloud cost for this experiment were zero. Experiments were
announced through a post in Twitter and other social networks, and
results were published here \cite{DBLP:conf/gecco/GuervosG15}. For the
purpose of this paper, we repeated the announcement several times
through the month of April and then by the beginning of August. All
in all, we have the set of runs with the characteristics shown in
Table \ref{tab:summary:os}. In general, every experiment took several
days. No particular care was taken about the time of the announcement
or the particular wording. Every {\em experiment} consisted in running
until the solution of the 40-trap problem was found. When the correct
solution was sent to the server, the counter was updated and the pool
of solutions reset to the void set. There was no special intention to wait
until all clients had finished, thus it might happen that, in fact,
the islands running in the browser {\em spill} from one experiment to
the next. However, previous experiments have proven that the influence
of these islands in the next experiment is indeed negligible.
%
\begin{table*}
\caption{Summary of time per run, number of IPs and number of PUTs per IP in the initial runs. \label{tab:summary:os}}
\begin{center}
\begin{tabular}{l|ccccccccc}
\hline
Date & Experiments &\#IPs &\#IPs &  \#IPs & Time (s) &  \#PUTs & $<$ 69s & $<$ 3.46s & Inter-experiment\\
     & \# & Different & Median & Max & Median &  Median & & & correlation \\ 
\hline
4/4 & 57 & 191 & 5 & 16 & 2040 & 18 & 14.29\% & 5.36\% & 0.0082 \\
4/24 & 231 & 559 & 5 & 29 & 732 & 11 & 47.39\% & 3.91\% & 0.0934\\
7/31 & 97 & 179 & 5 & 14 & 260 & 23 & 27.08\% & 1.04\%  & 0.1741\\
\hline
\end{tabular}
\end{center}
\end{table*}
%
The table shows that every run included more than 50 experiments. The
number of different IPs intervening in them varied from more than one
hundred to more than five hundred in the second experiment.

A summary of the results of each run is also shown in Table
\ref{tab:summary:os}, which shows the median number of IPs
intervening in each experiment,  median time needed
to finish the experiment, median number of HTTP PUTs per IP, and
then two columns showing the percentage of experiments that took less
than the two baseline experiments shown in the introduction to this
section. The first striking result is that in all cases, 50\% of the
experiments involved 5 or less IPs. This is consistent with results
previously obtained \cite{DBLP:conf/gecco/GuervosG15} which found 6 to be
the usual number of IPs that participated in an experiment. The
maximum number of different IPs for each experiment is also in the
same range and of the order of 10, which is also consistent with
prior work. The median time has a big range of variation, but 50\% of
the time takes less than several minutes, from around 4 minutes in the
best case to roughly 2/3 of an hour in the worst case. That figure is
not competitive, {\em a priori}, with the baseline experiment, that is
why it is interesting to look at the columns which measure
the percentage of experiments in which the solution was found in less
time than the baseline. It is similar in all three cases, with around
20\% on average for the first baseline experiment, roughly 3\% in the
second case. The comparison is not totally fair: this experiment takes
place in a browser, and in fact most of the time is devoted to
presenting the charts; the server is also not the same since it is run in a
PaaS instead of a high-performance desktop computer, and it is also a
remote server, not a local server as in the baseline experiment. But
even in this case, we were looking for creating a framework that
allowed us to run evolutionary computation experiments {\em faster} than
in our very own computer. And the results indicate that although it will
happen in 1 out of roughly 5 or roughly 20 cases, depending on what baseline
experiment you choose, it will not happen always or even most of
the time. One of the problems is shown in the last column, which
indicates the statistical correlation between the number of IPs
participating in one experiment and the next. In all cases,
correlation is so low as to affirm that there is almost no
relationship between them, that is, computing nodes are not {\em
  staying} after the solution has been found. This is a feature of
this implementation: the user has to actively reload the page to make
the evolutionary algorithm start again. Thus, the user can decide if to 
continue sharing resources or not. % (Paloma) Feature or disadvantage?
However, it puts a probably            % (Mario) Feature if you only want to share your CPU a little  
unnecessary upper limit on the time, or number of experiments, that a user
will voluntarily perform.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.32\linewidth]{time-vs-ips-OS-4-4.png}
\includegraphics[width=0.32\linewidth]{time-vs-ips-OS-4-24.png}
\includegraphics[width=0.32\linewidth]{time-vs-ips-OS-7-31.png}
\caption{Duration of experiments vs. number of different IPs (nodes)
  participating in it, with averages and standard deviation shown as
  black dots and lines; in the case there is a single dot, there was a single
  experiment in which so many computers participated (for instance, 16
  computers in the experiment in the far left or 29 in the middle
  one). 
Shade of gray or color indicate how many experiments included that many unique IPs,
so lighter (bluer) shade for a column of dots indicates that a particular number
of computers happened less frequently, while darker shade or closer to
red means more frequency. 
From left to right, experiments 4/4, 4/24 and 7/31.}
\label{fig:duration}
\end{figure}
% ../data/time-vs-IPs-openshift.R
%
\begin{figure}[!htb]
\centering
\includegraphics[width=0.32\linewidth]{time-vs-rank-OS-4-4.png}
\includegraphics[width=0.32\linewidth]{time-vs-rank-OS-4-24.png}
\includegraphics[width=0.32\linewidth]{time-vs-rank-OS-7-31.png}
\caption{Duration of experiments vs. rank, with $y$ axis in a
  logarithmic scale. Dot color (or gray value) is related to the number of IPs
  participating in the experiment. From left to right, experiments
  4/4, 4/24 and 7/31.} 
\label{fig:zipf:os}
% computed via ../data/time-vs-IPs-openshift.R
\end{figure}
%
We will have to analyze experimental data a bit further to find out why
this happens and also if there are some patterns in the three sets of
experiments. An interesting question to ask, for instance, is if
by adding more computers makes the experiment take less. In fact, as
shown in Figure \ref{fig:duration}, the {\em addition} of more computers does
not seem to contribute to decreasing the time needed to finish the
experiment. However, the cause-effect relationship is not clear at
all. It might be the opposite: since experiments take longer to finish
and might in fact be abandoned with no one contributing for some time,
that increases the probability of someone new joining them. In fact,
with experiments taking a few seconds and due to the way the
experiments are announced, it is quite difficult that several
volunteers join in in such a short period of time, even more if we take
into account that volunteers are not {\em carried over} from previous
experiments. This implies that it would be convenient to use a problem
of a bigger size to check this hypothesis as we have done in the next
experiment. 
%  however, at this point we
% have not found this convenient since there are several other issues
% that have to be solved, as it will be shown next. %Creo que este p√°rrafo
%saldr√≠a sobrando ahora que hicmios un experimento m√°s grande

\begin{figure}[!htb]
\centering
\includegraphics[width=0.32\linewidth]{puts-openshift-4-4.png}
\includegraphics[width=0.32\linewidth]{puts-openshift-4-24.png}
\includegraphics[width=0.32\linewidth]{puts-openshift-7-31.png}
\caption{Number of PUTs per unique IP and fit to a Generalized
  Extreme Value distribution (in a lighter shade or blue). From left to right, experiments
  4/4, 4/24 and 7/31.} 
% Plotted with ../data/plot-zipf-openshift.R
\label{fig:puts:os}
\end{figure}
%
It is also interesting to check the distribution of the experiment
duration, shown in Figure \ref{fig:zipf:os} and which roughly follows
a Zipf's law, with similar distribution along all three runs. The 4/24
run is the most complete and shows a S-shape, which implies an
accumulation of experiments taking similar time and around 100
seconds. The most interesting part is the {\em tail}, which shows how
many experiments took a desirable amount of time, of the order of
10 seconds, and which appears in all three graphs. As it can be seen,
it sharply drops implying there are 
just a few of them, and with diminishing probability as time
decreases. This exponential distribution also appears in the
distribution of HTTP PUTs, equivalent to the number of
generations divided by 100, contributed by every user, which is shown
in Figure \ref{fig:puts:os}. These results show a Zipf-like behavior,
so we have fitted it to the Generalized Extreme Value distribution,
with the resulting parameters shown in Table \ref{tab:puts:os}.
%
\begin{table*}
\caption{Summary of fit to Generalized Extreme Value distribution of
  the number of PUTs per unique IP. \label{tab:puts:os}}
\begin{center}
\begin{tabular}{l|ccc}
\hline
Date  & Location $\mu$ & Scale $\sigma$ & Shape $\xi$ \\
\hline
4/4 &  8.541 $\pm$ 1.0926  &    12.442 $\pm$ 1.7302 &  1.388 $\pm$
0.1377 \\
4/24 & 6.148 $\pm$ 0.3782 & 7.354 $\pm$ 0.5105 & 1.090 $\pm$  0.0697  \\
7/31 & 11.645 $\pm$ 1.475 & 16.365 $\pm$ 2.201 &  1.265 $\pm$ 0.132   \\
\hline
\end{tabular}
\end{center}
\end{table*}
%
This distribution was originally proposed to fit extreme values
\cite{resnick2013extreme} and contains, as a special case, the inverse
Weibull distribution which was fitted to volunteer computing
frameworks such as SETI@home \cite{javadi2009mining}. We obviously do
not pretend to compare our framework in scale or complexity with it, but
to point out that the behavior of volunteer computing nodes follows a
certain pattern, found in SETI@home, and which also appears in our framework. This
distribution is governed by three parameters, the usual location $\mu$
which is related to where it has its {\em center} and a scale $\sigma$,
related to the size, but also a third shape $\xi$ parameter that is
related to its skewness, that is, how skewed it is around the central
location. Positive parameters indicate that the distribution {\em
  leans} towards the origin, and negative ones towards the other extreme
value. In this case, Table \ref{tab:puts:os} shows $\xi$ values
greater than one and between one and 1.4, which indicates that the
three experiments share this origin-leaning pattern, with many users
donating a few cycles and just a few donating extreme values. Random
distributions with these parameters have been plotted in red in
Figure \ref{fig:puts:os}, indicating that the fit is good enough. The
predictive value of these fits, however, is limited, over all taking
into account the low correlation between successive events shown
above.

At any rate, this also shows that a convenient way of increasing the
computing power would be increasing this minimum amount per user. This
issue led us to create the next version of the framework, which is
presented next. % (Paloma) This sentences are sort of alone here... Could we extend a bit the first "this" to connect them to the previous paragraph ?

\begin{table*}[!htb]
\caption{Summary of {\sf NodIO-W$^2$} and comparison with the previous
  experiments, which have been aggregated to compute central measures. \label{tab:summary:ww}}
\begin{center}
\begin{tabular}{l|ccccccc}
\hline
Date & Median \#IPs & Max \#IPs & Median time (s) & Median \# PUTs & $<$ 69s & $<$ 3.46s & Inter-experiment correlation\\
\hline
{\sf NodIO} & 5 & 29 & 123 & 14 & 37.43\% & 3.40\% & 0.10 \\
{\sf NodIO-W$^2$} & 4  & 16 & 7.36 & 40 & 89\% & 36.90\% & 0.4336061 \\
\hline
\end{tabular}
\end{center}
\end{table*}
%
\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\linewidth]{ips-time-ww.png}
\caption{Time employed in every experiment vs. number of IPs (in
  abscissas). The black dot and line show the average and standard
  deviation, the blue to red shades the number of experiments with the same
  number of IPs. } 
\label{fig:ipstime:w2}
% ../data/time-vs-IPs.R 
\end{figure}

\subsection{Experiments using web workers}
\label{sec:w2}

The results reported above indicated that we should look for a way of
increasing the time that every user devotes. This led us to create an
architecture that uses Web Workers, which is described in
\cite{2016arXiv160101607M}; web workers work in the background even if
the web page is not in focus. A few improvements were also tested,
basically using a random population and restarting the web worker when
the solution was found.

The experiments were run in the same way as before, using several
 announcements in Twitter throughout several days by the end of July
2015. Eventually, more than one thousand experiments were completed in
a matter of days. A summary of results is shown in Table
\ref{tab:summary:ww}, comparing it with the aggregate of the results
obtained with the initial version of {\sf NodIO}. These results are
remarkably different, being similar only in the median and maximum
number of IPs, although in this case it would be combination
IP-worker, as we considered every worker as a unique IP. The median
number of seconds is almost 80\% better than the previous version, and
the median number of PUTs is three times better; this causes that most
of experiments take less time than one of the baselines, and one third
less than the second baseline. Thus, the conclusion in this case is
that we can obtain, through volunteer computation, a result that most 
of the times is better than we would by using a similar, non-parallel,
desktop setup, which makes this system suitable for massively
distributed evolutionary algorithms.
%
\begin{table*}
\caption{Summary of fit to GEV and Weibull distribution of
  the number of PUTs per worker. \label{tab:puts:ww}}
\begin{center}
\begin{tabular}{cccc}
\hline
Distribution & Location $\mu$ & Scale $\sigma$ & Shape $\xi$ \\
\hline
GEV & 18.275 $\pm$ 0.83719  &  25.962  $\pm$ 1.22015 & 1.242   $\pm$
0.04746 \\
Weibull & ND & 75.11457519 $\pm$ 2.97470251  & 0.69714410 $\pm$ 0.01383976 \\
\hline
\end{tabular}
\end{center}
\end{table*}
%

However, it is interesting to note why this is so, and the first hint
is the inter-experiment correlation between the number of IPs in
successive experiments. While before it was an unremarkable 10\%, it is
now more than 43\%, making the number of IPs in contiguous experiments
highly correlated. The main reason for this is the setup in {\sf
  NodIO-W$^2$}, which restarts an island after a solution has been
found and also that it keeps running even if the tab is not on the
foreground. This means that volunteers can leave the experiment
running for as long as they want and they will be contributing 
experiment after experiment, unlike the previous version, where they
stopped contributing after one solution was found. This is also
reflected in the number of PUTs, which is the number of generations
divided by 100; more than 50\% of the volunteers run the experiment for more than
4000 generations. All things considered, this means than there are
many more computers {\em simultaneously} running, leading to this
almost 5-fold increase in running time.


We are also interested in measuring the scaling properties in this
model, that is, the relationship between the number of IPs
participating in an experiment and the time it takes to complete
it. This is shown in Figure \ref{fig:ipstime:w2}, which displays every
experiment in terms of time (in milliseconds) vs. the number of workers participating
in it. Although there is not a clear trend, the graph seems to say
that the time needed for finding the solution in this evolutionary
algorithm does not depend on the number of workers participating in
it. This does not mean that it is independent on the number of {\em
  simultaneous} users, which is probably the case although it is more
difficult to measure. There seems to be a trend towards a decreasing
standard deviation in the time, with more volunteers adding both {\em
  robustness} to an experiment, and certainty in the time it is taking. 
 However, more experiments would be needed to check this hypothesis.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.49\linewidth]{gev-fit-ww.png}
\includegraphics[width=0.49\linewidth]{weibull-fit-ww.png}
\caption{Ranked number of PUTs per worker and GEV fit (in light
  or blue color, left) 
and Weibull (in lighter or blue shade, right) for the {\sf NodIO-W$^2$} model.}  
\label{fig:gev:w2}
% File ../data/ips-puts.R
\end{figure}
%
As we did in the previous subsection, we have also fitted the number
of HTTP PUTs per worker to a  Generalized Extreme Value (GEV) function, with the result shown
in Table \ref{tab:puts:ww}. Comparing this table with what we obtained
in the previous experiment, Table \ref{tab:puts:os}, we find higher values
for the location $\mu$ and scale $\sigma$ parameters, accounting for
the higher number of contributions per volunteer that has been
previously observed. However, the shape $\xi$ parameter is
substantially similar, with a {\em bell} shape leaning towards the
origin, indicating a pattern of a few users with many contributions
and many with few contributions. If we plot the model and the data
side by side, as shown in Figure \ref{fig:gev:w2} we see that the fit
is not so tight as in the previous case, with the model overestimating
the number of individuals with a high number contributions. This is
probably due to the fact that in the previous case, the user needed to
voluntarily reload the page to contribute the most generations, with a
{\em phase change} between the people that did so and the people that
just ran the experiment once until completion. In this case, however, the
user just needs to let it run, without that phase change and thus we obtain a
more {\em egalitarian} distribution of contributions, which rather
corresponds to a Weibull function, as shown in Figure
\ref{fig:ipstime:w2} (right hand side). The fitted parameters for this
distribution, shown in Table \ref{tab:puts:ww}, show a shape parameter
equal to 0.69, a value that is remarkably similar to the 0.5 value
found for time devoted to games in Chambers et
al. \cite{chambers2005measurement}. It is important to note that, in
this case, the number of PUTs made per client does not correspond to a
uniform amount of computation, since 100 generations with a random
population belong, in each case, to different number of
operations. In other words, the same number of PUTs will correspond to
different number of operations in each case. However, since clients
are also different and take a different time in each case, it is
difficult to ascertain how this might have an influence in the
statistical distribution.

In general, this second version of the {\sf NodIO} framework and the
single experiment performed prove that it can be the foundation for a distributed
high-server-performance evolutionary computation platform,
providing reasonable algorithmic performance and being, in general,
easy to use with a straightforward modification of the fitness
function. 

\section{Working on a hard optimization problem}
\label{sec:rastrigin}

The fact that the solution of this particular problem was found in a
few seconds and restarts were necessary yielded interesting results,
but we needed to work on a harder problem, that is why created a new
client version that optimized Rastrigin function, a multimodal
floating-point problem used as a benchmark in evolutionary
algorithms. The set up for this function, and a comparison of the
baseline speed for JavaScript in an standalone experiment and other
languages is shown in \cite{2016arXiv160101607M}, where the details of
the implementation have also been extensively described. 

Specifically, this experiment was deployed in a virtual private server with 512MB of memory
running Ubuntu OS version 14.04 and hosted in DigitalOcean.com. Unlike previous
experiments where the optimum was expected to be found many times, 
this problem was expected to run constantly for many days finding only
sub-optimal approximations. In order to achieve this, the only change needed 
in the framework was to limit the number of chromosomes kept in the pool. 
Previously, every PUT request which included a new chromosome increased 
the size of the pool. But in a long-running experiment this has to be avoided
in order to not exhaust the available memory. In this experiment the size
of a chromosome was 1000 double precision numbers, so that considering this
and the memory available in the server, the pool was limited 
to 10,000 chromosomes.  In order to keep this size
constant, a random chromosome was pulled out of the pool before inserting a
new one, when the pool was full. For this problem {\sf NodEO}'s {\tt chromosome-float} 
and {\tt classic-ea-float} modules were used. The parameters for the EA 
were tournament size = 3 and  population size = 500, returning exchanging a
chromosome with the server every 100 generations. Again, there were two workers in each window.
As in previous experiments, a few calls to participation were published in social networks.


%
% \begin{figure}[!htb]
% \centering
% \includegraphics{rastrigin-fitness.png}
% \caption{Fitness of chromosomes sent to the server pool along time in
%   the $x$ axis.} 
% % Plotted with ../data/plot-rastrigin-fitness.R
% \label{fig:puts:rastrigin}
% \end{figure}
%
\begin{figure}[!htb]
\centering
\includegraphics[width=0.49\linewidth]{rastrigin-IPs.png}
\includegraphics[width=0.49\linewidth]{rastrigin-IPs-hour.png}
\caption{Number of unique IPs participating in the Rastrigin
  experiment per minute (left) and per hour (right)} 
% Plotted with ../data/plot-rastrigin-IPs.R
\label{fig:ips:rastrigin}
\end{figure}
%
\begin{table*}
\caption{Summary of fit to GEV and Weibull distribution of
  the number of PUTs per worker for the F15 (Rastrigin) function. \label{tab:puts:ww:f15}}
\begin{center}
\begin{tabular}{cccc}
\hline
Distribution & Location $\mu$ & Scale $\sigma$ & Shape $\xi$ \\
\hline
GEV & 35.759  &  343.336   & 9.877 \\
Weibull & ND & 29.002625061 $\pm$ 2.153034198  & 0.402393903 $\pm$ 0.007586468 \\
\hline
\end{tabular}
\end{center}
\end{table*}
%
\begin{figure}[!htb]
\centering
\includegraphics[width=0.49\linewidth]{gev-fit-ww-rastrigin-workers.png}
\includegraphics[width=0.49\linewidth]{weibull-fit-ww-rastrigin-workers.png}
\caption{Data and fit to GEV (left) and Weibull (right) of the number
  of PUTs per worker for the F15 (Rastrigin) function.}  
% Plotted with ../data/ips-puts-rastrigin.R
\label{fig:fit:rastrigin}
\end{figure}
%

The results for this new experiment have been extremely interesting
and quite different from the previous one. The most important
characteristic that set this experiment apart from the previous one is
the fact that it did not find the solution for the time it was
running, instead lowering, little by little, the fitness until it was
disconnected. The fitness of chromosomes sent by clients managed to
surpass the 9000 value by the end of the; and although initially there
was a high number of new nodes, this number decreases with time.

This is in part due to the fact that the presence of nodes is not
constant, as shown in Figure \ref{fig:ips:rastrigin}, which plots the
number of unique IPs participating in the experiment per minute (left)
and hour (right). The two graphs have the same shape: At the beginning there are very few or only
one, but later one there are peaks of almost 30 unique machine a
minute; these peaks appear again when new announcements are made in
Twitter or any other social networks, to go down to a few remaining
nodes later on. It is interesting to note that, at the most, more than
100 unique nodes were participating in the experiment during one hour;
also that there are sustained peaks during a day of 6-7 computers
every minute. This is due to the fact that the computation is
asynchronous, but also to the fact that different computers stay in
the simulation for a different period of time. This period has been
plotted in Figure \ref{fig:fit:rastrigin}, that plots the number of
PUTs per node, ranked by number, along with a fit to the two
distributions we have used before, GEV and Weibull; the parameters of
this fit are shown in Table \ref{tab:puts:ww:f15}. Unlike the previous
experiments, in which GEV fit was relatively good, in this case it is
quite obviously not. However, the fit to the Weibull distribution,
shown at the left of the image, is relatively good. As should be
expected, the shape and scale are different to the one in the previous
experiments with the Trap function, however, the shape is quite
similar (0.40 vs 0.69), being in both cases less than one, which
indicates a decreasing distribution of the same kind. In fact, the
difference in the scale parameter indicates that the difference
between the time devoted by a volunteer and the next is bigger in this
case. This is probably due to the fact that the duration of the
experiment is bigger, allowing for bigger differences in
behavior. While the first in the Trap function might have devoted a
few minutes and the second a few seconds less, the difference between
the first and second in this experiment might be in the scale of
hours. 

% (Paloma) Mario, isn't this the future work? Should it be in conclusions? If not, just ignore this comment.
There are several validity threats to the design of this study.
Participants of the experiments were mainly members of our 
own social networks, personal colleagues, friends 
and acquaintances, therefore to extend this work other options 
should be considered in order to include 
a wider audience with different motives for participation.
Another threat is that participants were anonymous so the amount of repeated 
participations and the effects this might have had in the outcome is not
known. Another issue is that the scalability of the system was not thoroughly tested
but for the amount of participants the basic configuration 
was more than adequate, then again more experiments are needed in order
to assess the capability both with more clients and other server
configurations. Although care was taken to include representative benchmarks
of real world problems, more experiments with a broader coverage 
should allow a better understanding of the practical limitations of
the architecture and the tools chosen.


In general, this experiment confirms what was found in the others in
several aspects: first, there is a regularity in the distribution of the
time assigned by volunteers to it, second, there is a good and certain
amount of volunteers that will become part of the distributed
computer, in some cases up to one hundred; also, in some cases there
will be users that will spend several hours, two orders of magnitude
more generations in this case than in the previous one. There is a
clear influence of the time to solution: a short time means that every
experiment will only gather a few volunteers, but also that they will
stay only for a short time. A long time will make users stay for
longer, but if it takes too long, they will eventually lose interest. 

We will discuss this and the rest of the findings of the paper in the
conclusions. 

%---------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

In this paper two versions of a client-server architecture for volunteer and distributed
evolutionary algorithms have been evaluated. The architecture uses the browser and has been generated using {\sf
  NodIO}, based on the {\sf NodEO} evolutionary algorithm library. Volunteers are {\em in the cloud}, as stated in the title,
since they are a {\em CPU as a service} for the persons running the
experiment. In fact, in this paper we have tried to put some figures
on the real size of that {\em cloud} and how it can be used standalone
if there is no alternative, or in conjunction with other local or
cloud-based methods to add computing power in a seamless way through
the pool that {\sf NodIO} creates. 

In order to establish a baseline performance, the evolutionary
algorithm was run in a desktop client-program written in JavaScript
using NodEO to solve the 40-trap function. The first experiments with
{\sf NodIO} proved that, although obtaining better performance than the
baseline was possible, it did not happen, mainly because of the
difficulty in carrying over other experiments to volunteers when the
experiment they were participating on was finished. This has been shown in the low
correlation between the number of IPs in successive experiments, and also resulted in a low number of generations allotted by users. 

In the second implementation, {\sf NodIO-W$^2$}, Web Workers
were used so that several clients per browser could be executed and
they could run in the background, so that the the number of
generations per user could be improved. The number of generations per
user increased to the point that baseline performance was improved in
most cases. The architectural changes led to a high correlation
between successive experiments, so that, even as the median number of
volunteers per experiment decreased, they were probably contributing
to the experiment at the same time, so that our objective was
achieved. In general, one of the first conclusions for this paper is
that the browser technologies should be used to its full extent so
that the user time is leveraged to its full potential, and this was
achieved with {\sf NodIO-W$^2$}. 

The second objective of this paper was to model the user behavior in a
first attempt to try and predict performance. As should be expected,
the model depends on the implementation, with contributions following
a GEV distribution in the case of {\sf NodIO} and a
Weibull distribution for {\sf NodIO-W$^2$}. These distributions are
close enough to each other and, in the second case, reflect the fact
that all time spent in the page is actually devoted to computing,
which is why the time spent (represented by the number of
contributions to the pool) follows a model quite similar to that found
for games or other online activities. The reverse might be true: if we
want to have returning users for the experiments, it is probable that
we should {\em gamify} the experience so that once they have done it
once, they might do it more times. In the spirit of Open Science, this
gamification might involve computing in real-time data such as the one
presented in this paper, while showing it in the same page. 

In general, linking and finding correlations between user choices and
performance is an interesting avenue to explore in the future. Even if
the three previous experiments were published in a similar way, one
obtained up to 5 times more total cycles  than the one with the least
number of cycles. It is also essential to obtain volunteers as
simultaneously as possible, so it is possible that the features of the
social network in terms of real-time use will also play a big
role. Even as it is difficult to create controlled experiments in this
area, it is an interesting challenge to face in the future. % (Paloma) Wrong collocation

Although we think that, in general, the results presented in this
paper are independent of the problem chosen, it is true that even if
it is a difficult problem for evolutionary algorithms, it takes a few
seconds with the right settings to solve. This makes it difficult also
for this kind of asynchronous framework, but it would be interesting
to check the performance for other computationally demanding
tasks, especially ones that cover the middle ground between a problem
finishing in seconds or minutes (Trap) and weeks (Rastrigin F15). 
Besides, the two versions of the framework include as an
algorithmic variant using random population size. We do not think this
has had a big influence on the results and in fact this is not noticed
for F15, which is the second version tested, but it would be interesting to
measure exactly what this influence has been. In general there are
many issues with the evolutionary algorithm implementation itself,
including using different, or adaptive, policies for inserting and
sending individuals to the pool as was done in \cite{araujo2008mam},
using different policies for population initialization, and also the
incorporation of high-speed local resources to the pool to check what
would be the real influence of the volunteer pool to the final
performance. 

Another area is how to enhance the number and quality of
volunteers. For instance, adding a bit of more
control to the user might contribute also to gamification. Right now
the user has only two Web Workers. It is a matter of a single click to
open more tabs in the browser, giving more Web Workers, but it would be
interesting to put everything in a single page and under our control,
to check how often this happens and under which conditions. 

Finally, the implementation needs some refinement in terms of
programming and also ease of use. Tools such as Yeoman might be used % ¬øa√±adir una cita o la URL http://yeoman.io/ a pie de p√°gina ?
to create a generator in which the user just has to create a fitness
function, with the rest of the framework wrapped around
automatically.
%  All this, as well as the data used for this paper and the paper
% itself, has been published with a free license in GitHub at
% \url{https://github.com/JJ/modeling-volunteer-computing}.  

%---------------------------------------------------------------
\section*{Acknowledgment}

This work has been supported in part by
TIN2014-56494-C4-3-P (Spanish Ministry of Economy and Competitivity), PROY-PP2015-06 (Plan Propio 2015 UGR), 
SPIP2014-01437 (Direcci{\'o}n General de Tr{\'a}fico) and PYR-2014-17
GENIL project (CEI-BIOTIC Granada). Additional support was recieved by
Projects 5622.15-P (ITM) and  PROINNOVA 2015: 220590 (CONACYT).
We would also like to thank the
anonymous reviewers of previous versions of this paper who have really
helped us to improve 
this paper (and our work) with their suggestions. We are also grateful
to Anna S\'aez de Tejada for her help with the data processing scripts.

\bibliographystyle{abbrv}
\bibliography{geneura,volunteer,javascript,ror-js,GA-general}

\end{document}
%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% End:
