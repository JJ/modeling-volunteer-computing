\documentclass[runningheads,a4paper]{llncs}

\usepackage[latin1]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{subfigure}
\usepackage{url}
\usepackage{caption}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\providecommand{\tabularnewline}{\\}

\graphicspath{{../img/}}
\DeclareGraphicsExtensions{.pdf}


\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
library("ggplot2")
experiment.data <- data.frame(Experiment=character(),
                              milliseconds=character(),                           
                              IPs=character(), 
                              actualIPs=character(),
                              PUTs=character(),
                              actualPUTs=character(),
                              PUTratio = character(),
                              PUTsIP =character())

these.experiments <- c("nooverlap-1","nooverlap-newgraph-1", "nooverlap-newgraph-2","nooverlap-newgraph-reboot","nooverlap-newgraph-reboot-cachecrash")

experiment.aggregates <- data.frame(Experiment=character(),
                                    milliseconds=character(),
                                    IPs=character(),
                                    actualIPs = character(),
                                    PUTs = character(),
                                    actualPUTs = character(),
                                    count = character(),
                                    totalIPs = character(),
                                    PUTsIP = character())

experiment.aggregates.filtered = experiment.aggregates

ips.data <- data.frame(Experiment=character(),                      
                       PUTs=character(),
                       reboots=character(),
                       idu=character(),
                       idx=character())

ips.aggregates <- data.frame(Experiment=character(),
                             PUTs = character(),
                             reboots = character(),
                             avgPUTs = character(),
                             avgreboots = character(),
                             totalIPs = character())

for ( i in these.experiments ) {
    this.data <- read.csv(paste0("../data/2016-PPSN/ips-time-cache-2016-03-cache=32-",i,".csv"))

    this.ip.data <- read.csv(paste0("../data/2016-PPSN/ips-reboots-cache=32-",i,".csv"))
    this.len <- length(this.ip.data$PUTs)
    seq <- 1:this.len
    ips.data <- rbind(ips.data,
                      data.frame(Experiment=rep(i,length(this.ip.data$PUTs)),
                                 idu=seq,
                                 idx=seq/this.len,
                                 PUTs=as.integer(this.ip.data$PUTs),
                                 reboots=as.integer(this.ip.data$reboots))
                       )
    
    ips.aggregates <- rbind( ips.aggregates,
                            data.frame(Experiment=i,
                                       PUTs = median(this.ip.data$PUTs),
                                       reboots = median(this.ip.data$reboots),
                                       avgPUTs = mean(this.ip.data$PUTs),
                                       avgreboots = mean(this.ip.data$reboots),
                                       totalIPs =this.len )
                            )
    
    experiment.aggregates <- rbind( experiment.aggregates,
                                   data.frame( Experiment=i,                   
                                              milliseconds=median(as.integer(this.data$milliseconds)),
                                              IPs=median(as.integer(this.data$IPs)),
                                              actualIPs=median(as.integer(this.data$actualIPs)),
                                              PUTs=median(as.integer(this.data$PUTs)),
                                              actualPUTs=median(as.integer(this.data$actualPUTs)),
                                              PUTratio=median(as.integer(this.data$actualPUTs)/as.integer(this.data$PUTs)),
                                              count= length(this.data$IPs),
                                              totalIPs=this.len,
                                              PUTsIP=median(as.integer(this.data$PUTs)/as.integer(this.data$IPs)))
                                   )
    
    experiment.data <- rbind( experiment.data,
                             data.frame( Experiment=rep(i, length(this.data$IPs)),                   
                                        milliseconds=as.integer(this.data$milliseconds),
                                        IPs=as.integer(this.data$IPs),
                                        actualIPs=as.integer(this.data$actualIPs),
                                        PUTs=as.integer(this.data$PUTs),
                                        actualPUTs=as.integer(this.data$actualPUTs),
                                        PUTsIP=as.integer(this.data$PUTs)/as.integer(this.data$IPs)))

    this.data <- this.data[this.data$actualPUTs>=16,]
    experiment.aggregates.filtered <- rbind( experiment.aggregates.filtered,
                                            data.frame( Experiment=i,                   
                                                       milliseconds=median(as.integer(this.data$milliseconds)),
                                                       IPs=median(as.integer(this.data$IPs)),
                                                       actualIPs=median(as.integer(this.data$actualIPs)),
                                                       PUTs=median(as.integer(this.data$PUTs)),
                                                       actualPUTs=median(as.integer(this.data$actualPUTs)),
                                                       count= length(this.data$IPs),
                                                       PUTsIP=median(as.integer(this.data$PUTs)/as.integer(this.data$IPs))
                                                       )
                                            )

}

experiment.data$newgraph <- grepl("newgraph",experiment.data$Experiment)
experiment.data$nooverlap <- grepl("nooverlap",experiment.data$Experiment)
experiment.data$reboot <- grepl("reboot",experiment.data$Experiment)
experiment.data$cachecrash <- grepl("cache",experiment.data$Experiment)

time.data <- data.frame(Experiment=character(),
                        minute=as.Date(character()),
                        IPs=character())

these.experiments <- c("nooverlap-0","nooverlap","nooverlap-newgraph","nooverlap-newgraph-reboot-cachecrash")
ip.correlations <- data.frame(Experiment=character(),
                              correlation=character())

for ( i in these.experiments ) {
    this.data <- read.csv(paste0("../data/2016-PPSN/ips-per-minute-cache=32-",i,".csv"))
    this.data.plus1 <- this.data[2:length(this.data$IPs),]
    ip.correlations <- rbind(ip.correlations, 
                             data.frame(Experiment=i,
                                        correlation = cor(this.data[c(1:length(this.data$IPs)-1),]$IPs
                                                         ,this.data.plus1$IPs)))
    time.data <- rbind(time.data,
                       data.frame(Experiment=rep(i,length(this.data$IPs)),
                                  minute=strptime(this.data$time,"%Y-%m-%d %H:%M"),
                                  IPs=as.integer(this.data$IPs))
                       )
}


@

\title{A performance assessment of evolutionary algorithms in a volunteer computing system}

\author{Juan-J.~Merelo \and Paloma de las Cuevas \and Pablo
  Garc\'ia-S\'anchez \inst{1} \and Mario Garc\'ia-Valdez \inst{2}}

\institute{Dept. of Computer Architecture and Technology, University
of Granada, Spain \and
Dept. of Graduate Studies at Instituto Tecnol\'ogico de Tijuana}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
In a volunteer distributed computing system, users run a program in
their own machine to contribute to a common effort. If the program is
embedded in a web page, collaboration is easy, but also ephemeral. In
this paper we analyze a volunteer evolutionary computing system called
NodIO and try to find out a few rules that encourage volunteer
participation and how to make the most of their contributions to the
evolutionary algorithm. We will show different measures of
participation and contribution to the algorithm and how different
volunteer usage patterns and tweaks in the algorithm contribute to
improvements and leveraging of these contributions. 
\end{abstract}

\section{Introduction}
\label{introduction}


The world wide web not only provides a platform for content
distribution, but also an increasingly reliable and high-performance
platform for running distributed applications. This is based in two
key facts: the Javascript virtual machine every browser runs and a
simplified standard interface for interacting with servers. Thus
creating a distributed computing experiment is just a matter of making
a JavaScript application interchange information with a server, called
REST. From the point of view of the programmer, this involves
relatively common skills and no special libraries, since that
interface is built in the browser, and a simple application that
responds to those requests on the server side; both involve just a few
dozens lines of code additionally to whatever business logic your
application has. But, more importantly and from the point of view of
the user, that application can be run by simply visiting a web page. 

Using this system for creating distributed experiments is called {\em
  volunteer}, {\em cycle-scavenging} or {\em opportunistic} computing
\cite{sarmenta2001volunteer} and it dates back, in different shapes
and underlying mechanisms, to the origin of the web \cite{david-seti:home}. Our interest
here, however, is to use it as a resource for evolutionary
computation. This has been a line of research in our group for a long
time \cite{jj-ppsn98}, but has lately been revisited thanks to the
ubiquity of social networks and handheld computing devices. 

In this line of research that uses volunteer computing for
evolutionary algorithms, there are several pending issues. The first
and maybe most important is approaching volunteer computing as a
techno-social system \cite{vespignani2009predicting} which integrates
user decisions and behavioral patterns in the system model; this
includes trying to optimize the number of users in a particular
experiment. The second line of research, although related to the
first, is more focused on the evolutionary algorithm itself and how
different design decisions will affect its performance.

In this paper our focus will be in the second aspect: we will try to
design a decentralized system that, at the same time, is able to use
all resources available. This design will be done incrementally by
changing the system on the client and the server and measuring its
impact on the overall performance: time and evaluations needed to find
the solution. Eventually, we want to find a system that, whatever the
number of users available to perform the experiment, is able to
maximize their contribution to the evolutionary algorithm, at the same
time that the evolutionary algorithm itself makes the most of those
contributions, independently of how many they are.

The rest of the paper is organized as follows: Next we will briefly
present the state of the art in opportunistic distributed evolutionary
computation (EC). Section \ref{sec:description} will describe the
framework and problem used in the experiments, which are publicly
available under a free license. We will present the results of the
different steps in the incremental design in Section
\ref{sec:experiments}, to finally wrap up with the conclusions. 

%---------------------------------------------------------------
\section{State of the art}
\label{sec:soa}

Volunteer computing involves a user running a program voluntarily
and, as such, has been deployed in many different ways from the
beginning of the Internet, starting with the SETI@home framework for
processing extraterrestrial signals \cite{david-seti:home}. However
the dual introduction of JavaScript as a universal language for the
browser and the browser itself as an ubiquitous web and Internet client has
made this combination the most popular for volunteer computing
frameworks such as the one we are using here, and whose first version
was described in \cite{DBLP:conf/gecco/GuervosG15}.

JavaScript can be used for either unwitting
\cite{unwitting-ec,boldrin2007distributed} or volunteer 
\cite{langdon:2005:metas,gecco07:workshop:dcor} distributed
evolutionary computation and it has been used ever since by several
authors, including more recent efforts
\cite{duda2013distributed,DBLP:journals/corr/abs-0801-1210,EvoStar2014:jsEO}. 

In general, those papers go no further than a proof of concept,
attempting to gauge how many users join the effort and also how long
would it take a concrete amount of users to reach the solution. In
general, they do not try to assess the performance of the algorithm
itself in the precise conditions they find in a volunteer computing
system, although papers such as the one by Laredo et
al. \cite{churn08:ijhpsa}, using models, try to find out the resulting
performance when the users are changing continously according to a
Weibull distribution. In the case of Klein and Spector \cite{unwitting-ec}, the algorithm
is actually run on the server, using the browser mainly for fitness
evaluation so no actual contribution to the evaluation of the
evolutionary algorithm itself is made.

There is another factor that should be taken into account: since the
user has control of the browser, there is a limited amount of
interaction with it, namely, the fact that by reloading the webpage he
can apply a kind of hypermutation killing the current population and
generating new individuals some of which will make their way to the
common pool via migration. In that sense, volunteer computing is also
a way of {\em human computation} \cite{quinn2011human}, a concept that
has also been applied to evolutionary algorithms \cite{972056}, in
this case extensively and with all operators. It is quite difficult to
find out how many times this happens and what is their effect on the
overall algorithm, but this only reaffirms the fact that there is a
long list of issues with volunteer evolutionary computation, and that
the volunteer him or herself is at the center of many of them.

In this paper we will make contributions towards the design of
efficient volunteer evolutionary algorithms by studying the effect of
several design decisions on performance. In the next Section we will
present the general framework and the initial setup.


\section{Description of the framework}
\label{sec:description}

In general, a distributed volunteer-based evolutionary computation
system based on the browser is simply a client-server system
with a client embedded in the browser via JavaScript. We call this
system {\sf NodIO}. All parts of the framework are free and available
with a free license from
\url{https://github.com/JJ/splash-volunteer}. 

The architecture of the {\sf NodIO} system is divided in two tiers:\begin{enumerate}
\item A REST (representational state transfer) server that responds to
  HTTP requests made by the browsers. These HTTP requests use JSON for
  encoding information, in this case chromosomes and associates
  information send by the clients and return requests made by the
  server.  The server has the capability to
  run a single experiment, storing the incoming chromosomes in a cache data structure
  that is reset when the solution is found. This data has a finite
  size that erases the oldest chromosomes once it has filled to
  capacity.  
\item A client that includes the evolutionary algorithm as
  JavaScript code embedded in a web page that displays the fitness and
  other graphs, some
  additional links, and information on the experiment. This code runs
  an evolutionary algorithm {\em island} that starts with a random
  population, then after every 100 generations, it sends the best individual
  back to the server (via a {\tt PUT} request), and then requests a random
  individual back from the server (via a {\tt GET} request). We have
  kept the number of generations between migrations fixed since it is
  a way of finding out how much real work every client is doing. In
  the version of the server used in this paper, we implemented a
  policy by which the combination IP-fitness value could only be
  inserted once into the cache. That means that if a particular IP
  sends the fitness 99 into the server, it will be accepted only
  once. The main intention of this policy was to avoid {\em
    overlapping} of contributions, with clients still running an {\em
    old} run of the experiment contributing after a new one had
  started. This did not completely avoid these contributions: if the
  clients of the {\em old} run increased their new fitness, it would
  still be allowed. 
  % As I remember we kept the run_number so IPs with old runs
  % did not interfere with newer ones. But you are saying: what if they are super slow 
  % and always PUT when the run is already over?, we want to receive 
  % those individuals? 
  % "That means that if a particular IP
  % sends the fitness 99 into the server, it will be accepted only
  % once."  
  % once per run?
% JJ - once per run or experiment. But we didn't keep that run_number
% to avoid interference, just to keep track
  %  but different individuals could have the same fitness, are we limiting the search? 
% We are actually checking for the same individual, not only the fitness
  %  maybe we could have multiple versions of the pool a la MVCC  :)
% Let's leave that for the future...- JJ
  % I think if we are just trying to limit overlapping of the experiments
  % then PUTs should send the run's seq_number and completly remove it.  
% The client does not keep track of the seq number. Maybe we should do
% just that, but sending back and forth the seq number implies
% refactoring. - JJ
\end{enumerate}

Figure \ref{fig:system} describes the general system architecture and
algorithm behavior. Different JavaScript libraries, such as JQuery or {\tt
  Chart.js} have been used to build the user interface elements of the
framework, which should be running in
\url{http://nodio-jmerelo.rhcloud.com}, a free resource hosted in the
OpenShift Platform as a Service.\footnote{It is not guaranteed to be running,
or running the same version, when you read this, however; you can always get
the sources from GitHub and set it up yourself.}
%
\begin{figure}[!t]
\centering
\includegraphics[width=3in]{system.pdf}
\caption{Description of the proposed system. Clients execute a JavaScript EA
  in the browser, which, every 100 generations, sends the best
  individual and receives a random one back from the server.}
\label{fig:system}
\end{figure}

NodIO needs a fitness function to work with; in this case the classical Trap 
function \cite{Ackley1987} has been used; this function runs in the
browser, but the server needs to know when the solution has been
found, so a termination condition is also included as a configuration
option in the server. In all cases we are using 50 Traps, a problem
that is difficult enough to need the intervention of several users for
an extended amount of minutes.

For every new individual sent by the clients to the cache, several
% We have not presented the "cache",  only the server and
% ..storing the incoming chromosomes in a data structure
% Im changing over there to cache
pieces of meta data were stored: a time stamp, the client IP, the chromosome and
value, the cache size in that particular moment, and also if that
individual actually updated the cache or not. 

All experiments are announced in the same way: we use social networks,
mainly Twitter, to announce the start of a new experiment. LinkedIn,
Facebook, and also private groups in Telegram and WhatsApp have been
used too for announcements. We also encourage friends and followers to
put an original post for announcing it, instead of just sharing, in
the way allowed by the platform, the original post. During the running
of the experiments, users are engaged, questions answered, and all
kind of explanations given if requested. In general, announcements
have been made in Spanish, but in some cases English was used
too. This makes for a certainly unrealiable experimentation framework,
but it is a realistic one. The steps in the design of the evolutionary
algorithm will be explained next.

%---------------------------------------------------------------
\section{Experiments and results} 
\label{sec:experiments}

\begin{table}[htb]
\caption{Experiment table, with summary of results. \label{tab:runs}}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
Experiment & Median Time & \multicolumn{2}{|c|}{Median IPs} &
\multicolumn{3}{|c|}{Median PUTs} & \#Runs & Unique IPs & Puts/IP \\
           &   (ms)        & Total & Used & Total & Used & Ratio &  & & \\
\hline
base & 120056.0 & 10 & 4 & 256.0 & 21.0 & 0.1290323 & 81 & 262 & 21.33333\\
\hline
graph1 & 152619.5 & 9 & 4 & 212.5 & 26.5 & 0.1282238 & 36 & 96 & 24.30000\\

graph2 & 1173561.5 & 6 & 4 & 761.5 & 22.0 & 0.0520509 & 36 & 73 & 129.04545\\
\hline
reboot & 796214.0 & 9 & 6 & 646.0 & 67.0 & 0.0955121 & 45 & 168 & 75.88889\\
\hline
cachereboot & 392917.0 & 12 & 12 & 576.0 & 79.0 & 0.1575092 & 55 & 225 & 43.28571\\
\hline
\end{tabular}
%% \end{tabular}
%% Initially generated with this code, 
% <<experiments, cache=FALSE,echo=FALSE>>=
%experiment.aggregates$Experiment <- c("base","graph1","graph2","reboot","cachereboot")
%kable(experiment.aggregates)
%@ %def 
\end{center}
\end{table}
%
%
\begin{table}
  \caption{Experiments with overlaps eliminated:  summary of results. \label{tab:filtered}}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
Experiment & Median Time & \multicolumn{2}{|c|}{Median IPs} &\multicolumn{2}{|c|}{Median PUTs} & \#Runs &  Puts/IP \\
&   (ms)        & Total & Used & Total & Used &  & \\
\hline
base & 293685 & 13.0 & 8.0 & 483.0 & 57.0 & 47 & 35.50000\\
\hline
graph1 & 768053 & 12.0 & 5.0 & 601.0 & 79.0 & 21 & 45.84615\\
graph2 & 1477969 & 6.0 & 4.0 & 901.0 & 52.0 & 25 & 149.42857\\
\hline
reboot & 799471 & 9.5 & 6.5 & 656.5 & 67.0 & 44 & 76.48016\\
\hline
cachereboot & 463347 & 12.0 & 12.0 & 638.5 & 80.5 & 52 & 44.25911\\
\hline
\end{tabular}
%% Initially generated with this code, 
%%<<experiments.filtered, cache=FALSE,echo=FALSE>>=
%%experiment.aggregates.filtered$Experiment <- c("base","graph1","graph2","reboot","cachereboot")
%%kable(experiment.aggregates.filtered)
%%@ %def 

\end{center}
\end{table}
%
%
\begin{figure}[htbp]
\centering

<<zipf, cache=FALSE,echo=FALSE,fig.height=3>>=
ggplot(ips.data,aes(x=idx,y=PUTs,group=Experiment,color=Experiment))+geom_point()+scale_y_log10()+ scale_colour_discrete(labels=c('base','graph1','graph2','reboot','cachereboot'))+scale_x_log10()
@ %def graph for IPs and puts

\caption{Ranked and normalized number of contributions (PUTs) per IP,
  plotted in a log $y$ scale.}
\label{fig:zipf}
\end{figure}

Since the point of using volunteer computing system is to cut costs,
all experiments were set up using the OpenShift
PaaS, which provides a free tier, making the whole experiment cost
equal to \$0.00. In fact, the NodIO system can be deployed to any
system as long as it can run node.js and access to the filesystem, for
log files, is available. All these logs are available under
rrequest. Processed files are available in the same repository that
hosts this paper, as well as the scripts needed to process them. 

Every run had slightly different conditions, although in some cases it
was just a server reboot and a new round of announcements. Each run
batch included at least 30 competions, that is, running until the
solution was found. The number of runs was controlled by polling an
URL that indicated the number of runs made in the present batch so
far. A summary of the experiments and  its results is shown in Table
\ref{tab:runs}. This table shows the median time needed to find the
solution as well as the median number of IPs and PUTs. In this case we
distinguish between the total number of clients participating in that
particular run, and the {\em actual} number whose contributions were
accepted into the cache, which was set up to accept those only if {\em
  new} chromosomes were sent to the server in order to avoid overlaps
between one run and the next. That is why, in both cases, the
``Contributing'' column includes a value that is less than the total
number. The final column includes the total number of runs in that
particular batch, finished after a minimum number of 30 was reached. 

The baseline, called {\em base} in the Table, uses simply the
mechanism of dropping contributions from clients if they are repeated
to avoid overlap. Clients still get a random chromosome from the
server, so they can in fact proceed with the algorithm and even finish
it. The fact that these users were not contributing to the pool was
conveyed by inserting a graph in the client that represented the size
of the cache and labeled ``How am I contributing''. The user could
then realize if his contribution was being 0 and thus do something
about it; these are labeled {\tt graph1} and 2. However, a stuck
client could still add to the pool if evolution got unstuck, so we
added a mechanism for rebooting the client, that is, reloading the
page, if the cache size was found to be less than 1, which indicated a
run recently initiated. This batch of runs is labelled {\tt
  reboot}. However, we found that this size of the cache was actually
skipped by some slow clients or too fast evolutions, so that in the
last case, labeled {\tt cachecrash}, clients rebooted if the cache
collapsed by more than a certain length, indicating a cache that had
been voided and was filling again. Our objective was twofold: to
encourage engagement by the users and also to increase the involvement
of every client in the common experiment, eliminating at the same time
overlaps between runs. 

So we will have to check whether we have effectively eliminated these
overlaps. In order to do that, we will have to establish a threshold
under which reasonably the solution cannot be found. In every
evolutionary algorithm it might happen, by chance, that the solution
is found in just a few evaluations, but that is usually not the
case. By looking at the logs in the last case we have established that
minimun as 16 PUTs; in less than 1600 generations no solution to that
difficult problem will be found. Let us filter then the runs,
eliminating those that use less than 1600 generations to find the
solution which can reasonably be said that they did so because some
browser ``carried over'' a chromosome from the previous run. The
results are shown in Table \ref{tab:filtered}








%---------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

Our intention in this paper was to assess the capabilities of a
sociotechnical system formed by a client-server web-based framework
running a distributed evolutionary algorithms and the volunteers that
participate in the experiment. These volunteers are {\em in the
  cloud}, that is, available as {\em CPU as a service} for the persons
running the experiment. In this paper we have tried to put some figures
on the real size of that {\em cloud} and how it can be used standalone
if there is no alternative, or, if other computing resources are
available, in conjunction with other local or cloud-based methods to
add computing power in a seamless way through the pool that NodIO creates. 

After running the experiment on the 40-trap problem whose running time
could be as low as a few seconds, we switched to another batch of
experiments where we used the 50-trap problem and also to a pool of
limited, and dwindling through the three experiments, size. Since our
initial results indicated that what happened on the screen, a flat
graph with no improvements or the experiment finished, influenced the
amount of time that the users devoted to the experiment, a longer one
could yield different results and, at the same time, result in a big
pool that might either crash the server or return useless individuals
to the volunteers. These new experiments have proved that using the
limited pool is beneficial to finding the solution, since less
evaluations are needed, but also that since the problem is more
difficult, the users stay for longer in the web page, making less
ephemeral the sociotechnical computing system created by the
simulation. 


The second objective of this paper was to model the user behavior in a
first attempt to try and predict performance. As should be expected,
the model depends on the implementation, with contributions following
a Weibull distribution, which reflects the fact
that volunteer computing follows a model quite similar to that found
for games or other online activities. The reverse might be true: if we
want to have returning users for the experiments, it is probable that
we should {\em gamify} the experience so that once they've done it
once, they might do it more times. In the spirit of Open Science, this
gamification might involve computing in real time data such as the one
presented in this paper and showing it in the same page or presenting
user results alongside others.

In general, linking and finding correlations between user choices and
performance is an interesting avenue to explore in the future. Even if
these  experiments were published in a similar way, one
obtained up to 5 times more total cycles than the one with the least
number of cycles. It is also essential to obtain volunteers as fast and
simultaneously as possible, so it is possible that the features of the
social network in terms of real-time use will also play a big
role; synchronous webs such as Snapchat, which mystifies the writers
of this paper, and Twitter, thanks to its real time nature, might be
better suited than Facebook, LinkedIn or Google plus. Even as it is
difficult to create controlled experiments in this 
area, it is an interesting challenge to explore in the future.

The other area to explore is the algorithmic area itself. Are there
ways to change the evolutionary algorithm, or its visualization, so
that the user has a bigger impact on the result? One of the users in
Twitter even suggested to embed videos so that people spent time
looking at them, but other possible way was to make the user engage
the algorithm by giving him or her buttons to change the mutation rate
when the algorithm is stalled, for instance. If this is combined with
a score board where local performance is compared to other users,
engagement might be increased and thus the performance of the
system. In general there are 
many issues with the evolutionary algorithm implementation itself,
including using different, or adaptive, policies for inserting and
sending individuals to the pool,
using different policies for population initialization, and also the
incorporation of high-speed local resources to the pool to check what
would be the real influence of the volunteer pool to the final
performance. 

Finally, the implementation needs some refinement in terms of
programming and also ease of use. Tools such as Yeoman for generating
easily new experiments might be used, so that the user would have  to
create only a fitness
function, with the rest of the framework wrapped around
automatically. 

All these avenues of experimentation will be done openly following the
Open Science policy of our group, which, in fact, contributes to
establish trust and security between us and volunteers and is an
essential feature of the system. That is why this paper, as well as
the data and processing scripts, are published with a free license in GitHub at
\url{https://github.com/JJ/modeling-volunteer-computing}.

%---------------------------------------------------------------
\section*{Acknowledgments}

This work has been supported in part by TIN2014-56494-C4-3-P (Spanish Ministry of Economy and Competitivity),
PROY-PP2015-06 (Plan Propio 2015 UGR)  % please, remember to include all the current projects   ;)   Thanks!
. We would also like to thank the
anonymous reviewers of previous versions of this paper who have really
helped us to improve 
this paper (and our work) with their suggestions. We are also grateful
to Anna S\'aez de Tejada for her help with the data processing
scripts. We are also grateful to {\tt @otisdriftwood} for his help
gathering users for the new experiments. 


\bibliographystyle{splncs}
\bibliography{volunteer,GA-general,geneura,javascript,ror-js}

\end{document}

%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% End:
